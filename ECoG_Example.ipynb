{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now let's build a loader for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aopy import datareader, datafilter\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file:\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re(down)sample data\n",
    "# srate = 250 # not much signal above 100Hz\n",
    "# ds_factor = np.intc(np.floor(srate_in/srate)) # decimate does not allow for floats as ds_fac arguments\n",
    "# data_in = sp.signal.decimate(data_in,ds_factor,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EcogDataset(Dataset):\n",
    "    def __init__(self, data_in, block_len):\n",
    "        self.data = data_in\n",
    "        self.block_len = int(block_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] // self.block_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = int(index)\n",
    "        return self.data[idx:(idx + self.block_len),:] # each call returns a sequence of length `block_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "srate = srate_in\n",
    "# data_in = np.double(data_in[:,:120*srate])\n",
    "seq_len = 11 # use ten time points to predict the next time point\n",
    "# dataset = pd.DataFrame(data_in.transpose(),dtype=np.double) # may be unnecessary for now, but df will probably help combine files in the future.\n",
    "# datareader.load_ecog_clfp_data.get_args()\n",
    "data_tensor = torch.from_numpy(data_in.transpose())\n",
    "print(data_tensor.dtype)\n",
    "dataset = EcogDataset(data_tensor,seq_len) ## make my own Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "block_time = 20\n",
    "block_size = block_time*srate\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "data_size = np.shape(data_in)[-1]\n",
    "n_block = np.floor(data_size/block_size)\n",
    "idx_all = np.arange(data_size)\n",
    "train_split = int(np.floor(train_frac*data_size))\n",
    "valid_split = int(np.floor(valid_frac*data_size))\n",
    "test_split = int(np.floor(test_frac*data_size))\n",
    "# rework this using\n",
    "train_idx = idx_all[0:train_split:seq_len]\n",
    "valid_idx = idx_all[train_split:train_split+valid_split:seq_len] \n",
    "test_idx  = idx_all[train_split+valid_split:-1:seq_len]\n",
    "# print(train_idx, valid_idx)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=train_sampler,\n",
    "                                           drop_last=True) # this can be avoided using some padding sequence classes, I think\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                          sampler=test_sampler,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 62])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if your DataLoader is working correctly, this should be `torch.Size([<seq_len>, <num_ch>])\n",
    "dataset.__getitem__(next(iter(train_sampler))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to our data (potenially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data, test_data),\n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # gated recurrent layer, dropout layer\n",
    "        self.gru_layer = nn.GRU(input_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        # note: batch_first only permutes dimension order in input and output tensors. It does not affect hidden state.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # input_data: [batch_size x seq_len x input_dim]\n",
    "        # h0: [n_layers x batch_size x hid_dim]\n",
    "        batch_size = input_data.size(0)\n",
    "#         hidden = torch.randn(self.n_layers, batch_size, self.hid_dim) # initialize hidden layer value\n",
    "        output, hidden = self.gru_layer(input_data) # hidden initialized as zero tensor\n",
    "            \n",
    "        # output = [batch_size x seq_len x hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # input = [batch_size, seq_len, hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "#         input_data = input_data.unsqueeze(0) # not sure if this this is needed for not-embedded inputs\n",
    "        if len(input_data.size()) != 3 or len(hidden.size()) != 3:\n",
    "            breakpoint()\n",
    "        output, hidden = self.rnn(input_data, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #\"seq len and n directions will always be 1 in the decoder, therefore:\" <- figure out how to change this\n",
    "        #output = [batch_size, 1, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, output, hidden # predicted ECoG signal, decoder states, last decoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device # what is this?\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Encoder, decoder embedding dimensions (hidden state) must be equal.\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder, decoder layer number must be equal.\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio: prob. to use teacher forcing\n",
    "        #e.g. if 0.75, ground-truth imports are used 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_dim = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_dim).to(self.device)\n",
    "        \n",
    "        enc_state, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:,0,:] # this was grabbing indexing into the first batch element, not the first sequence element.\n",
    "        \n",
    "        for t in range(1,trg_len): # ignore that first data point\n",
    "            pred, output, hidden = self.decoder(input,hidden)\n",
    "            \n",
    "            outputs[t] = pred\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            input = trg[:,t,:] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SEQ_LEN = 10 \n",
    "OUTPUT_SEQ_LEN = 1 # predict one output state from 10 inputs prior\n",
    "INPUT_DIM = num_ch\n",
    "OUTPUT_DIM = num_ch\n",
    "HID_DIM = num_ch\n",
    "N_ENC_LAYERS = 1 \n",
    "N_DEC_LAYERS = 1\n",
    "ENC_DROPOUT = np.float32(0.5)\n",
    "DEC_DROPOUT = np.float32(0.5)\n",
    "\n",
    "enc = Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq_GRU(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 62]) torch.Size([1, 3, 62])\n",
      "torch.Size([3, 11, 62]) torch.Size([3, 11, 62]) torch.Size([1, 3, 62])\n"
     ]
    }
   ],
   "source": [
    "# example of enc/dec function\n",
    "# let's pass the first pop off the dataset to the encoder and look at the outputs\n",
    "enc_out, hid_enc = enc.forward(train_loader.__iter__()._next_data())\n",
    "# out: [h1, h2, ..., h{seq_len}]\n",
    "# hid: h{seq_len}\n",
    "print(enc_out.size(),hid_enc.size())\n",
    "est, dec_out, hid_dec = dec.forward(enc_out,hid_enc)\n",
    "print(est.size(),dec_out.size(),hid_dec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq_GRU(\n",
       "  (encoder): Encoder_GRU(\n",
       "    (gru_layer): GRU(62, 62, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder_GRU(\n",
       "    (rnn): GRU(62, 62, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=62, out_features=62, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        src = batch[:,:-1,:]\n",
    "        trg = batch[:,-1,:].unsqueeze(1) # otherwise it would automatically cut this out.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #### RESUME DEBUGGING HERE - 2020.03.05.1800\n",
    "        breakpoint()\n",
    "        ####\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch\n",
    "            trg = batch\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq_GRU(\n",
      "  (encoder): Encoder_GRU(\n",
      "    (gru_layer): GRU(62, 62, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder_GRU(\n",
      "    (rnn): GRU(62, 62, batch_first=True, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=62, out_features=62, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1a24d19550>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1a24d196d0>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(train_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-27-776a09230dcb>(20)train()\n",
      "-> output_dim = output.shape[-1]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  output.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 62])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-27-776a09230dcb>(22)train()\n",
      "-> output = output[1:].view(-1, output_dim)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-27-776a09230dcb>(23)train()\n",
      "-> trg = trg[1:].view(-1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  trg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.5712e+00,  1.7371e+00, -3.7617e+01, -1.2642e+01, -1.9826e+00,\n",
      "           1.5354e+01,  8.2303e+00,  4.7060e+01,  4.7313e+00, -8.0275e+00,\n",
      "          -8.5970e+00,  6.8349e+00,  7.0171e+01, -6.4290e+00,  5.0932e+01,\n",
      "           1.0116e+02,  5.5229e+01,  5.3648e+01,  6.8373e+01,  4.9456e+01,\n",
      "           9.6087e+01,  1.1446e+02,  1.2144e+02,  1.5584e+02,  1.4867e+02,\n",
      "           4.6781e+01,  3.5955e+01,  2.5189e+02, -1.4065e+01,  1.5370e+02,\n",
      "           2.8549e+02,  2.8922e+02,  2.9503e+02,  2.5611e+02,  2.8940e+02,\n",
      "           2.1902e+02,  2.3649e+02,  9.4047e+01,  3.8417e+01,  3.7832e+01,\n",
      "           2.0239e+02,  8.7640e+01,  1.3835e+02,  5.1044e+01,  4.9968e+00,\n",
      "           3.3520e+01,  1.5415e+02,  1.1280e+02,  1.4710e+02,  3.8994e+01,\n",
      "           4.8086e+01,  6.3107e+01,  1.5108e+02,  1.2302e+02,  1.1168e+02,\n",
      "           5.5282e+01,  5.5433e+01, -5.8299e+00,  2.6748e+01,  4.4082e+01,\n",
      "          -1.5590e+01,  3.8389e+01]],\n",
      "\n",
      "        [[-7.0955e+03, -7.4616e+03, -7.3266e+03, -7.5778e+03, -7.6896e+03,\n",
      "          -7.4681e+03, -6.9694e+03, -7.7225e+03, -7.2368e+03, -7.5954e+03,\n",
      "          -7.7077e+03, -6.5793e+03, -6.9988e+03, -6.8240e+03, -7.5980e+03,\n",
      "          -7.2285e+03, -7.4358e+03, -7.3538e+03, -7.8593e+03, -7.5465e+03,\n",
      "          -7.1981e+03, -7.3031e+03, -7.6488e+03, -7.5468e+03, -7.6613e+03,\n",
      "          -6.1578e+03, -7.0091e+03, -6.7165e+03, -7.0063e+03, -5.3721e+03,\n",
      "          -7.2578e+03, -7.3914e+03, -7.4411e+03, -7.3409e+03, -7.6939e+03,\n",
      "          -6.3799e+03, -7.8394e+03, -7.1054e+03, -7.7275e+03, -7.4674e+03,\n",
      "          -7.8316e+03, -7.2623e+03, -6.9040e+03, -6.9856e+03, -7.6478e+03,\n",
      "          -7.1106e+03, -8.1844e+03, -7.5618e+03, -7.9699e+03, -6.2361e+03,\n",
      "          -6.9980e+03, -7.6740e+03, -7.9871e+03, -8.1646e+03, -6.9317e+03,\n",
      "          -7.1121e+03, -7.5958e+03, -6.6762e+03, -5.2754e+03, -4.0566e+03,\n",
      "          -3.8383e+03, -4.2510e+03]],\n",
      "\n",
      "        [[ 5.4357e+01,  1.0125e+02,  4.2759e+01,  9.1482e+01,  6.0298e+01,\n",
      "           1.0234e+02,  5.9451e+01,  8.7033e+01,  1.0325e+02,  1.1936e+02,\n",
      "           5.6406e+01,  1.5602e+01,  1.1036e+02,  6.4467e+01,  7.3737e+01,\n",
      "           1.0879e+02,  1.1828e+02,  8.4545e+01,  1.3535e+02,  1.0235e+02,\n",
      "           1.5703e+02,  1.6585e+02,  1.5740e+02,  1.9440e+02,  1.8875e+02,\n",
      "           7.4093e+01,  9.0668e+01,  1.6383e+02, -1.2702e+01,  3.0155e+01,\n",
      "           2.0438e+02,  2.2665e+02,  1.9394e+02,  1.7891e+02,  1.6654e+02,\n",
      "           1.5357e+02,  1.7374e+02,  1.6157e+02,  1.1673e+02,  1.3373e+02,\n",
      "           1.2898e+02,  9.0326e+01,  9.4316e+01,  5.4189e+01,  1.2149e+02,\n",
      "           3.8160e+01,  1.0685e+02,  1.0332e+02,  5.6145e+01,  3.0798e+01,\n",
      "           1.0774e+02,  8.9638e+01, -7.5469e+00,  8.1179e+01,  3.8039e+00,\n",
      "           4.3631e+01,  8.2732e+01,  5.5588e+01, -2.3632e+01,  3.2172e+01,\n",
      "           4.3432e+00, -2.0063e+01]]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "> <ipython-input-27-776a09230dcb>(23)train()\n",
      "-> trg = trg[1:].view(-1)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  trg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.5712e+00,  1.7371e+00, -3.7617e+01, -1.2642e+01, -1.9826e+00,\n",
      "           1.5354e+01,  8.2303e+00,  4.7060e+01,  4.7313e+00, -8.0275e+00,\n",
      "          -8.5970e+00,  6.8349e+00,  7.0171e+01, -6.4290e+00,  5.0932e+01,\n",
      "           1.0116e+02,  5.5229e+01,  5.3648e+01,  6.8373e+01,  4.9456e+01,\n",
      "           9.6087e+01,  1.1446e+02,  1.2144e+02,  1.5584e+02,  1.4867e+02,\n",
      "           4.6781e+01,  3.5955e+01,  2.5189e+02, -1.4065e+01,  1.5370e+02,\n",
      "           2.8549e+02,  2.8922e+02,  2.9503e+02,  2.5611e+02,  2.8940e+02,\n",
      "           2.1902e+02,  2.3649e+02,  9.4047e+01,  3.8417e+01,  3.7832e+01,\n",
      "           2.0239e+02,  8.7640e+01,  1.3835e+02,  5.1044e+01,  4.9968e+00,\n",
      "           3.3520e+01,  1.5415e+02,  1.1280e+02,  1.4710e+02,  3.8994e+01,\n",
      "           4.8086e+01,  6.3107e+01,  1.5108e+02,  1.2302e+02,  1.1168e+02,\n",
      "           5.5282e+01,  5.5433e+01, -5.8299e+00,  2.6748e+01,  4.4082e+01,\n",
      "          -1.5590e+01,  3.8389e+01]],\n",
      "\n",
      "        [[-7.0955e+03, -7.4616e+03, -7.3266e+03, -7.5778e+03, -7.6896e+03,\n",
      "          -7.4681e+03, -6.9694e+03, -7.7225e+03, -7.2368e+03, -7.5954e+03,\n",
      "          -7.7077e+03, -6.5793e+03, -6.9988e+03, -6.8240e+03, -7.5980e+03,\n",
      "          -7.2285e+03, -7.4358e+03, -7.3538e+03, -7.8593e+03, -7.5465e+03,\n",
      "          -7.1981e+03, -7.3031e+03, -7.6488e+03, -7.5468e+03, -7.6613e+03,\n",
      "          -6.1578e+03, -7.0091e+03, -6.7165e+03, -7.0063e+03, -5.3721e+03,\n",
      "          -7.2578e+03, -7.3914e+03, -7.4411e+03, -7.3409e+03, -7.6939e+03,\n",
      "          -6.3799e+03, -7.8394e+03, -7.1054e+03, -7.7275e+03, -7.4674e+03,\n",
      "          -7.8316e+03, -7.2623e+03, -6.9040e+03, -6.9856e+03, -7.6478e+03,\n",
      "          -7.1106e+03, -8.1844e+03, -7.5618e+03, -7.9699e+03, -6.2361e+03,\n",
      "          -6.9980e+03, -7.6740e+03, -7.9871e+03, -8.1646e+03, -6.9317e+03,\n",
      "          -7.1121e+03, -7.5958e+03, -6.6762e+03, -5.2754e+03, -4.0566e+03,\n",
      "          -3.8383e+03, -4.2510e+03]],\n",
      "\n",
      "        [[ 5.4357e+01,  1.0125e+02,  4.2759e+01,  9.1482e+01,  6.0298e+01,\n",
      "           1.0234e+02,  5.9451e+01,  8.7033e+01,  1.0325e+02,  1.1936e+02,\n",
      "           5.6406e+01,  1.5602e+01,  1.1036e+02,  6.4467e+01,  7.3737e+01,\n",
      "           1.0879e+02,  1.1828e+02,  8.4545e+01,  1.3535e+02,  1.0235e+02,\n",
      "           1.5703e+02,  1.6585e+02,  1.5740e+02,  1.9440e+02,  1.8875e+02,\n",
      "           7.4093e+01,  9.0668e+01,  1.6383e+02, -1.2702e+01,  3.0155e+01,\n",
      "           2.0438e+02,  2.2665e+02,  1.9394e+02,  1.7891e+02,  1.6654e+02,\n",
      "           1.5357e+02,  1.7374e+02,  1.6157e+02,  1.1673e+02,  1.3373e+02,\n",
      "           1.2898e+02,  9.0326e+01,  9.4316e+01,  5.4189e+01,  1.2149e+02,\n",
      "           3.8160e+01,  1.0685e+02,  1.0332e+02,  5.6145e+01,  3.0798e+01,\n",
      "           1.0774e+02,  8.9638e+01, -7.5469e+00,  8.1179e+01,  3.8039e+00,\n",
      "           4.3631e+01,  8.2732e+01,  5.5588e+01, -2.3632e+01,  3.2172e+01,\n",
      "           4.3432e+00, -2.0063e+01]]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/utils/data/sampler.py(203)__iter__()\n",
      "-> yield batch\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  ste\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'ste' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorExit\n",
      "> /Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/utils/data/sampler.py(198)__iter__()\n",
      "-> def __iter__(self):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> /Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/utils/data/sampler.py(124)<genexpr>()\n",
      "-> return (self.indices[i] for i in torch.randperm(len(self.indices)))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object SubsetRandomSampler.__iter__.<locals>.<genexpr> at 0x1a98278150>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/utils/data/sampler.py\", line 124, in <genexpr>\n",
      "    return (self.indices[i] for i in torch.randperm(len(self.indices)))\n",
      "  File \"/Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/bdb.py\", line 90, in trace_dispatch\n",
      "    return self.dispatch_call(frame, arg)\n",
      "  File \"/Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/bdb.py\", line 135, in dispatch_call\n",
      "    if self.quitting: raise BdbQuit\n",
      "bdb.BdbQuit: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7777deec54c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-776a09230dcb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#trg = [(trg len - 1) * batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    test_loss = evaluate(model.double(), test_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(torch.Tensor(np.arange(10)).size()) != 3:\n",
    "    breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
