{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now let's build a loader for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aopy import datareader, datafilter\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file:\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot data segment for a nice tidy figure\n",
    "# import matplotlib.pyplot as pp\n",
    "# t_plot = np.arange(data_in.shape[1])/srate_in\n",
    "# t_plot.shape\n",
    "\n",
    "# t_start = 70\n",
    "# t_end = 80\n",
    "# plot_idx = range(srate_in*t_start,srate_in*t_end)\n",
    "# n_chan_plot = 10\n",
    "# ch_idx = range(n_chan_plot)\n",
    "\n",
    "# fig, ax = pp.subplots(figsize=(3,8))\n",
    "# ax.plot(data_in[0:n_chan_plot*6:6,plot_idx].transpose() + 1500*np.arange(n_chan_plot),t_plot[plot_idx])\n",
    "# ax.set_ylim((t_start,t_end))\n",
    "# fig.patch.set_visible(False)\n",
    "# ax.set_xlabel('ECoG Data')\n",
    "# ax.axis('off')\n",
    "\n",
    "# with open(\"ECoG_trace.png\", 'wb') as outfile:\n",
    "#     fig.canvas.print_png(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re(down)sample data\n",
    "# srate = 250 # not much signal above 100Hz\n",
    "# ds_factor = np.intc(np.floor(srate_in/srate)) # decimate does not allow for floats as ds_fac arguments\n",
    "# data_in = sp.signal.decimate(data_in,ds_factor,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EcogDataset(Dataset):\n",
    "    def __init__(self, data_in, block_len):\n",
    "        self.data = data_in\n",
    "        self.block_len = int(block_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] // self.block_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = int(index)\n",
    "        return self.data[idx:(idx + self.block_len),:] # each call returns a sequence of length `block_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "srate = srate_in\n",
    "# data_in = np.double(data_in[:,:120*srate])\n",
    "enc_len = 10\n",
    "dec_len = 1\n",
    "seq_len = enc_len+dec_len # use ten time points to predict the next time point\n",
    "# dataset = pd.DataFrame(data_in.transpose(),dtype=np.double) # may be unnecessary for now, but df will probably help combine files in the future.\n",
    "# datareader.load_ecog_clfp_data.get_args()\n",
    "data_tensor = torch.from_numpy(data_in.transpose())\n",
    "print(data_tensor.dtype)\n",
    "dataset = EcogDataset(data_tensor,seq_len) ## make my own Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "block_time = 20\n",
    "block_size = block_time*srate\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "data_size = np.shape(data_in)[-1]\n",
    "n_block = np.floor(data_size/block_size)\n",
    "idx_all = np.arange(data_size)\n",
    "train_split = int(np.floor(train_frac*data_size))\n",
    "valid_split = int(np.floor(valid_frac*data_size))\n",
    "test_split = int(np.floor(test_frac*data_size))\n",
    "# rework this using\n",
    "train_idx = idx_all[0:train_split:seq_len]\n",
    "valid_idx = idx_all[train_split:train_split+valid_split:seq_len] \n",
    "test_idx  = idx_all[train_split+valid_split:-1:seq_len]\n",
    "# print(train_idx, valid_idx)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=train_sampler,\n",
    "                                           drop_last=True) # this can be avoided using some padding sequence classes, I think\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                          sampler=test_sampler,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 62])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if your DataLoader is working correctly, this should be `torch.Size([<seq_len>, <num_ch>])\n",
    "dataset.__getitem__(next(iter(train_sampler))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to our data (potenially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data, test_data),\n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # gated recurrent layer, dropout layer\n",
    "        self.gru = nn.GRU(input_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        # note: batch_first only permutes dimension order in input and output tensors. It does not affect hidden state.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # input_data: [batch_size x seq_len x input_dim]\n",
    "        # h0: [n_layers x batch_size x hid_dim]\n",
    "        batch_size = input_data.size(0)\n",
    "#         hidden = torch.randn(self.n_layers, batch_size, self.hid_dim) # initialize hidden layer value\n",
    "        output, hidden = self.gru_layer(input_data) # hidden initialized as zero tensor\n",
    "            \n",
    "        # output = [batch_size x seq_len x hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # input = [batch_size, seq_len, hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "#         input_data = input_data.unsqueeze(0) # not sure if this this is needed for not-embedded inputs\n",
    "        if len(input_data.size()) != 3 or len(hidden.size()) != 3:\n",
    "            breakpoint()\n",
    "        output, hidden = self.rnn(input_data, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #\"seq len and n directions will always be 1 in the decoder, therefore:\" <- figure out how to change this\n",
    "        #output = [batch_size, 1, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, output, hidden # predicted ECoG signal, decoder states, last decoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device # what is this?\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Encoder, decoder embedding dimensions (hidden state) must be equal.\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder, decoder layer number must be equal.\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio: prob. to use teacher forcing\n",
    "        #e.g. if 0.75, ground-truth imports are used 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        \n",
    "        src_len = src.shape[1]\n",
    "        src_dim = src.shape[2]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_dim = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)\n",
    "        \n",
    "        enc_state, hidden = self.encoder(src)\n",
    "        \n",
    "        output = src[:,-1,:].unsqueeze(1) # start the decoder with the actual output\n",
    "        \n",
    "        for t in range(trg_len): # ignore that first data point\n",
    "            pred, output, hidden = self.decoder(output,hidden)\n",
    "            \n",
    "            outputs[:,t,:] = pred.squeeze()\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            input = trg[:,t,:] if teacher_force else output\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SEQ_LEN = 10 \n",
    "OUTPUT_SEQ_LEN = 1 # predict one output state from 10 inputs prior\n",
    "INPUT_DIM = num_ch\n",
    "OUTPUT_DIM = num_ch\n",
    "HID_DIM = num_ch\n",
    "N_ENC_LAYERS = 1 \n",
    "N_DEC_LAYERS = 1\n",
    "ENC_DROPOUT = np.float32(0.5)\n",
    "DEC_DROPOUT = np.float32(0.5)\n",
    "\n",
    "enc = Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq_GRU(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 11, 62]) torch.Size([1, 3, 62])\n",
      "tensor([[[ 1.9925e-01,  1.5957e-02, -2.0756e-01,  ...,  2.5550e-01,\n",
      "          -1.7293e-01, -6.5301e-02],\n",
      "         [ 1.8119e-01, -1.4908e-01, -5.6814e-03,  ...,  6.5629e-02,\n",
      "          -6.8411e-02, -3.6459e-02],\n",
      "         [ 1.9039e-01, -2.2826e-01,  7.1456e-02,  ..., -4.8206e-02,\n",
      "          -3.5384e-02,  1.8990e-03],\n",
      "         ...,\n",
      "         [ 1.4772e-01, -1.1799e-01,  1.6614e-01,  ..., -1.2116e-01,\n",
      "          -7.9860e-02,  1.6674e-01],\n",
      "         [ 1.2995e-01, -8.4423e-02,  1.6582e-01,  ..., -1.0815e-01,\n",
      "          -6.9709e-02,  1.6483e-01],\n",
      "         [ 1.0737e-01, -6.0524e-02,  1.6267e-01,  ..., -9.2503e-02,\n",
      "          -6.6244e-02,  1.5733e-01]],\n",
      "\n",
      "        [[-1.0720e-01,  6.1206e-01, -9.4757e-02,  ..., -1.6421e-01,\n",
      "           1.1136e-01,  2.6572e-01],\n",
      "         [-6.4889e-02,  2.8313e-01, -4.4704e-02,  ..., -2.4483e-01,\n",
      "           1.2277e-01,  1.9054e-01],\n",
      "         [-3.4246e-02,  1.0895e-01,  6.2070e-03,  ..., -2.4907e-01,\n",
      "           1.4052e-01,  1.7424e-01],\n",
      "         ...,\n",
      "         [-4.6118e-02, -8.3176e-02,  4.0179e-02,  ..., -3.1575e-01,\n",
      "           9.9185e-02,  2.6334e-01],\n",
      "         [-1.8910e-01, -1.1631e-01, -1.6307e-02,  ..., -2.0009e-01,\n",
      "           1.6690e-01,  1.5095e-01],\n",
      "         [-2.3151e-01, -5.9762e-02, -3.8312e-02,  ..., -1.8616e-01,\n",
      "           1.6926e-01,  1.3541e-01]],\n",
      "\n",
      "        [[-6.9567e-01, -4.6803e-01,  3.5602e-01,  ..., -1.1871e-01,\n",
      "           3.6890e-01,  4.3618e-01],\n",
      "         [-4.5566e-01, -3.1076e-01,  1.2969e-01,  ..., -1.5046e-01,\n",
      "           2.8676e-01,  3.6622e-01],\n",
      "         [-3.6821e-01, -2.2142e-01,  4.2163e-04,  ..., -1.2746e-01,\n",
      "           1.4963e-01,  3.0542e-01],\n",
      "         ...,\n",
      "         [-1.3555e-01, -1.7607e-01, -1.1592e-01,  ..., -2.5165e-02,\n",
      "          -6.1862e-02,  8.1476e-02],\n",
      "         [-1.0648e-01, -1.9291e-01, -1.0653e-01,  ..., -1.2137e-03,\n",
      "          -7.3552e-02,  5.9432e-02],\n",
      "         [-8.7414e-02, -2.0333e-01, -1.0347e-01,  ...,  1.1243e-02,\n",
      "          -7.9873e-02,  4.8036e-02]]], grad_fn=<AddBackward0>) torch.Size([3, 11, 62]) torch.Size([1, 3, 62])\n"
     ]
    }
   ],
   "source": [
    "# example of enc/dec function\n",
    "# let's pass the first pop off the dataset to the encoder and look at the outputs\n",
    "enc_out, hid_enc = enc.forward(train_loader.__iter__()._next_data())\n",
    "# out: [h1, h2, ..., h{seq_len}]\n",
    "# hid: h{seq_len}\n",
    "print(enc_out.size(),hid_enc.size())\n",
    "est, dec_out, hid_dec = dec.forward(enc_out,hid_enc)\n",
    "print(est,dec_out.size(),hid_dec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq_GRU(\n",
       "  (encoder): Encoder_GRU(\n",
       "    (gru_layer): GRU(62, 62, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder_GRU(\n",
       "    (rnn): GRU(62, 62, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=62, out_features=62, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,778 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 62]) torch.Size([3, 1, 62])\n",
      "torch.Size([3, 1, 62])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(25138.0586, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the forward pass\n",
    "# get first data pull\n",
    "# dataset.__getitem__(next(iter(train_sampler)))\n",
    "data_batch = next(iter(train_loader))\n",
    "src = data_batch[:,:enc_len,:]\n",
    "trg = data_batch[:,enc_len:,:]\n",
    "print(src.size(),trg.size())\n",
    "test_out = model(src,trg)\n",
    "print(test_out.size()) # it actually works!\n",
    "criterion(test_out,trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        src = batch[:,:-1,:]\n",
    "        trg = batch[:,-1,:].unsqueeze(1) # otherwise it would automatically cut this out.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [batch size, trg len, output dim]\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "#         output = output[1:].view(-1, output_dim)\n",
    "#         trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch[:,:-1,:]\n",
    "            trg = batch[:,-1,:]\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq_GRU(\n",
      "  (encoder): Encoder_GRU(\n",
      "    (gru_layer): GRU(62, 62, batch_first=True, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder_GRU(\n",
      "    (rnn): GRU(62, 62, batch_first=True, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=62, out_features=62, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1a2334f4d0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1a2334f850>\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(train_loader)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    test_loss = evaluate(model.double(), test_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(torch.Tensor(np.arange(10)).size()) != 3:\n",
    "    breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
