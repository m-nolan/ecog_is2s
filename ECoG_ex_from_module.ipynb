{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aopy import datareader, datafilter\n",
    "from ecog_is2s import EcogDataloader, Training\n",
    "from ecog_is2s.model import Encoder, Decoder, Seq2Seq\n",
    "from ecog_is2s.model import Util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import progressbar as pb\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed RNG for pytorch/np\n",
    "SEED = 5050\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file:\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']\n",
    "# we already found the appropriate data masks, so just load them in\n",
    "mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "with open(mask_file_path, 'rb') as f:\n",
    "    mask_data = pkl.load(f)\n",
    "hf_mask = mask_data[\"hf\"]\n",
    "sat_mask = mask_data[\"sat\"]\n",
    "\n",
    "# mask data array, remove obvious outliers\n",
    "data_in[:,np.logical_or(hf_mask,sat_mask)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x1a209d5e10>\n"
     ]
    }
   ],
   "source": [
    "# create dataset object from file\n",
    "srate = srate_in\n",
    "# data_in = np.double(data_in[:,:120*srate])\n",
    "enc_len = 1000\n",
    "dec_len = 10\n",
    "seq_len = enc_len+dec_len # use ten time points to predict the next time point\n",
    "\n",
    "total_len_T = 10*60 # I just don't have that much time!\n",
    "total_len_n = total_len_T*srate\n",
    "data_idx = data_in.shape[1]//2 + np.arange(total_len_n)\n",
    "\n",
    "data_tensor = torch.from_numpy(sp.stats.zscore(data_in[:,data_idx].view().transpose()))\n",
    "print(data_tensor.size)\n",
    "dataset = EcogDataloader.EcogDataset(data_tensor,seq_len) ## make my own Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479192,) (0,) (119797,)\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "valid_frac = 0.0\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "idx_all = np.arange(dataset.data.shape[0])\n",
    "sample_idx = idx_all[:-seq_len]\n",
    "\n",
    "train_loader, test_loader, valid_loader = EcogDataloader.genLoaders(dataset, sample_idx, train_frac, test_frac, valid_frac, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,778 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# build the model, initialize\n",
    "INPUT_SEQ_LEN = enc_len\n",
    "OUTPUT_SEQ_LEN = dec_len # predict one output state from 10 inputs prior\n",
    "INPUT_DIM = num_ch\n",
    "OUTPUT_DIM = num_ch\n",
    "HID_DIM = num_ch\n",
    "N_ENC_LAYERS = 1 \n",
    "N_DEC_LAYERS = 1\n",
    "ENC_DROPOUT = np.float32(0.5)\n",
    "DEC_DROPOUT = np.float32(0.5)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "enc = Encoder.Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder.Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq.Seq2Seq_GRU(enc, dec, device).to(device)\n",
    "model.apply(Util.init_weights)\n",
    "\n",
    "print(f'The model has {Util.count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(479192,) (0,) (119797,)\n",
      "Training Network:\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_loss = np.zeros(N_EPOCHS)\n",
    "train_batch_loss = []\n",
    "test_loss = np.zeros(N_EPOCHS)\n",
    "test_batch_loss = []\n",
    "\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1,1,1)\n",
    "\n",
    "for e_idx, epoch in enumerate(range(N_EPOCHS)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get new train/test splits\n",
    "    train_loader, test_loader, _ = EcogDataloader.genLoaders(dataset, sample_idx, train_frac, test_frac, valid_frac, BATCH_SIZE)\n",
    "    \n",
    "    print('Training Network:')\n",
    "    train_loss[e_idx], trbl_ = Training.train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    train_batch_loss.append(trbl_)\n",
    "    print('Testing Network:')\n",
    "    test_loss[e_idx], tebl_ = Training.evaluate(model, test_loader, criterion)\n",
    "    test_batch_loss.append(tebl_)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = Util.epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss[e_idx] < best_test_loss:\n",
    "        best_test_loss = test_loss[e_idx]\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss[e_idx]:.3g}')\n",
    "    print(f'\\t Val. Loss: {test_loss[e_idx]:.3g}')\n",
    "    \n",
    "    ax.plot(e_idx,train_loss[e_idx],'b.')\n",
    "    ax.plot(e_idx,test_loss[e_idx],'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
