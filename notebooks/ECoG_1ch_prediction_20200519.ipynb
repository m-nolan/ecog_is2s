{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECoG Forecasting with Sequence-to-Sequence (seq2seq) RNN models.\n",
    "\n",
    "## Starting small: 1 channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from aopy import datareader, datafilter\n",
    "from ecog_is2s import EcogDataloader, Training, Encoder, Decoder, Seq2Seq, Util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# import progressbar as pb\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell is used to parse command line arguments. These arguments assign values to the network and training parameters. This functionality has been replaced with more hard-coded constants in the current notebook. From an organizational standpoint, that's not the worst thing - it's given me a good opportunity to collect all of the constants together into one single code block @ the top of the notebook. They were pretty scattershot before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grab input arguments\n",
    "# parser = argparse.ArgumentParser('Trains a seq2seq network on a section of example NHP PMC ECoG data.',add_help=True)\n",
    "# parser.add_argument('--encoder-depth', metavar='el', type=int, default=10, help='Sequence depth of the encoder network')\n",
    "# parser.add_argument('--decoder-depth', metavar='dl', type=int, default=1, help='Sequence depth of the decoder network')\n",
    "# parser.add_argument('--batch-size', metavar='b', type=int, default=1, help='Data batch size')\n",
    "# parser.add_argument('--num-epochs', metavar='n', type=int, default=1, help='Number of optimization epochs')\n",
    "# parser.add_argument('--num-layers', metavar='nl', type=int, default=1, help='Number of layers in each RNN block')\n",
    "\n",
    "# args = parser.parse_args() # this bad boy has all the values packed into it. Nice!\n",
    "# print(args.encoder_depth,args.decoder_depth)\n",
    "\n",
    "# print(args.encoder_depth,args.decoder_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO:\n",
    "Clean this mess up! There are lots of copied values and variable rereferences. They're unncessary and confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "ENCODER_DEPTH = 100\n",
    "DECODER_DEPTH = 10\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 200\n",
    "N_EPOCHS = NUM_EPOCHS\n",
    "NUM_LAYERS = 1\n",
    "RNG_SEED = 5050\n",
    "INPUT_SEQ_LEN = ENCODER_DEPTH\n",
    "OUTPUT_SEQ_LEN = DECODER_DEPTH\n",
    "N_CH_USE = 1\n",
    "N_LAYER = NUM_LAYERS\n",
    "N_ENC_LAYERS = N_LAYER\n",
    "N_DEC_LAYERS = N_LAYER\n",
    "ENC_DROPOUT = np.float32(0.5)\n",
    "DEC_DROPOUT = np.float32(0.5)\n",
    "LEARN_RATE = 0.01 # default ADAM: 0.001\n",
    "LOSS_OBJ = 'MSE' #L1, L2, see training.py:ECOGLoss()\n",
    "WEIGHT_RANGE = (-0.2,0.2) # ignore for now; not sure how to worm this through\n",
    "train_frac = 0.8\n",
    "test_frac = 0.2\n",
    "valid_frac = 0.0\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# N_EPOCHS = args.num_epochs\n",
    "CLIP = 1. # this the maximum norm of the whole parameter gradient.\n",
    "TFR = 0.0 # no teacher forcing! Anything it's learning is all on its own\n",
    "RAND_SAMP = False\n",
    "weight_reg = 5.\n",
    "enc_len = ENCODER_DEPTH\n",
    "dec_len = DECODER_DEPTH\n",
    "seq_len = ENCODER_DEPTH+DECODER_DEPTH # use ten time points to predict the next time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed RNG for pytorch/np\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "torch.cuda.manual_seed(RNG_SEED)\n",
    "torch.backends.cudnn.deterministic = True # enforces deterministic algorithm use -> reproducibility. Remove for production code. You don't do production code. Don't remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mounting to device: cpu\n"
     ]
    }
   ],
   "source": [
    "# set device - CUDA if you've got it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('mounting to device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file:\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "platform_name = sys.platform\n",
    "if platform_name == 'darwin':\n",
    "    # local machine\n",
    "    data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "    model_save_dir_path = '/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq/'\n",
    "elif platform_name == 'linux2':\n",
    "    # HYAK, baby!\n",
    "    data_file_full_path = '/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = \"/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "elif platform_name == 'linux':\n",
    "    # google cloud, don't fail me now\n",
    "    data_file_full_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.mask.pkl'\n",
    "    model_save_dir_path = '/home/mickey/models/pyt/seq2seq/'\n",
    "\n",
    "# make sure the output directory actually exists\n",
    "if not os.path.exists(model_save_dir_path):\n",
    "    os.makedirs(model_save_dir_path)\n",
    "\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling data from 1000 to 250\n",
      "Data Size:\t(62, 15000)\n",
      "\n",
      "Filtering Channels:\n",
      "Num. ch. used:\t56\n",
      "Ch. kept:\t[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57]\n",
      "\n",
      "Normalizing data, converting to tensor:\n",
      "Data tensor shape: torch.Size([15000, 56])\n",
      "\n",
      "Creating EcogDataloader dataset object:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mask data array, remove obvious outliers\n",
    "data_in[:,np.logical_or(data_mask[\"hf\"],data_mask[\"sat\"])] = 0.\n",
    "\n",
    "# downsample data\n",
    "srate_down = 250\n",
    "srate = srate_in\n",
    "\n",
    "# grab local time segment (way way less than the full file)\n",
    "total_len_T = 1*60 # I just don't have that much time!\n",
    "total_len_n = total_len_T*srate_in\n",
    "data_idx = data_in.shape[1]//2 + np.arange(total_len_n)\n",
    "print('Downsampling data from {0} to {1}'.format(srate_in,srate_down))\n",
    "### note: this breaks mask indexing, if you plan to do that later.\n",
    "data_in = np.float32(sp.signal.decimate(data_in[:,data_idx],srate_in//srate_down,axis=-1))\n",
    "print('Data Size:\\t{}\\n'.format(data_in.shape))\n",
    "\n",
    "# filter dead channels\n",
    "ch_rms = np.std(data_in,axis=-1)\n",
    "ch_m = np.mean(ch_rms)\n",
    "ch_low_lim = ch_m - 2*np.std(ch_rms)\n",
    "ch_up_lim = ch_m + 2*np.std(ch_rms)\n",
    "ch_idx = np.logical_and(ch_rms > ch_low_lim, ch_rms < ch_up_lim)\n",
    "ch_list = np.arange(num_ch)[ch_idx]\n",
    "num_ch_down = len(ch_list)\n",
    "print('Filtering Channels:')\n",
    "print('Num. ch. used:\\t{}'.format(num_ch_down))\n",
    "print('Ch. kept:\\t{}\\n'.format(ch_list))\n",
    "\n",
    "#create data tensor\n",
    "print('Normalizing data, converting to tensor:')\n",
    "data_rail = np.max(np.abs(data_in.reshape(-1)))\n",
    "# normalization = 'zscore'\n",
    "normalization = 'tanh'\n",
    "if normalization is 'max':\n",
    "    data_tensor = torch.from_numpy(data_in[ch_idx,:].view().transpose()/data_rail)\n",
    "elif normalization is 'zscore':\n",
    "    # for nominally gaussian data distributions, this will get ~99% of data points in (-1, 1)\n",
    "    data_tensor = torch.from_numpy(sp.stats.zscore(data_in[ch_idx,:].view().transpose())/5)\n",
    "elif normalization is 'tanh':\n",
    "    data_tensor = torch.from_numpy(np.tanh(sp.stats.zscore(data_in[ch_idx,:].view().transpose())/3))\n",
    "print('Data tensor shape: {}\\n'.format(data_tensor.shape))\n",
    "\n",
    "# create dataset object\n",
    "print('Creating EcogDataloader dataset object:')\n",
    "if device == 'cuda:0':\n",
    "    data_tensor.cuda()\n",
    "dataset = EcogDataloader.EcogDataset(data_tensor[:,:N_CH_USE],device,seq_len) ## make my own Dataset class\n",
    "num_ch_down = dataset.n_ch\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sampling index sets\n",
    "idx_all = np.arange(dataset.data.shape[0])\n",
    "idx_step = int(np.round(0.1*srate_down))\n",
    "sample_idx = idx_all[:-seq_len:idx_step]\n",
    "# plot samples\n",
    "n_plot_seed = 1\n",
    "n_plot_step = 4*seq_len\n",
    "plot_seed_idx = np.arange(0,n_plot_seed*n_plot_step,n_plot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,793 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mickey/anaconda3/envs/ecog_is2s/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# define the network!\n",
    "INPUT_DIM = num_ch_down\n",
    "OUTPUT_DIM = num_ch_down\n",
    "HID_DIM = 64*num_ch_down\n",
    "\n",
    "enc = Encoder.Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, INPUT_SEQ_LEN, ENC_DROPOUT)\n",
    "dec = Decoder.Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, OUTPUT_SEQ_LEN, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq.Seq2Seq_GRU(enc, dec, device).to(device)\n",
    "model.apply(Util.init_weights)\n",
    "\n",
    "print(f'The model has {Util.count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARN_RATE,weight_decay=weight_reg)\n",
    "criterion = Training.ECOGLoss(sum_axis=1,objective=LOSS_OBJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple progressbar, not tied to the iterator\n",
    "def print_progress_bar(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving session data to:\t/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq/enc100_dec10_nl1_nep200_20200519215148347707\n",
      "[===================================================---------] 85.5% ...epoch:\t171\r"
     ]
    }
   ],
   "source": [
    "# train/test!\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_loss = np.zeros(N_EPOCHS)\n",
    "train_batch_loss = []\n",
    "test_loss = np.zeros(N_EPOCHS)\n",
    "test_batch_loss = []\n",
    "\n",
    "# create training session directory\n",
    "time_str = Util.time_str() # I may do well to pack this into util\n",
    "session_save_path = os.path.join(model_save_dir_path,'enc{}_dec{}_nl{}_nep{}_{}'.format(enc_len,dec_len,N_ENC_LAYERS,N_EPOCHS,time_str))\n",
    "sequence_plot_path = os.path.join(session_save_path,'example_sequence_figs')\n",
    "os.makedirs(session_save_path) # no need to check; there's no way it exists yet.\n",
    "os.makedirs(sequence_plot_path)\n",
    "print('saving session data to:\\t{}'.format(session_save_path))\n",
    "# save a histogram of the data distribution; allowing you to check\n",
    "f,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "ax.hist(dataset.data.reshape(-1),100,density=True)\n",
    "f.savefig(os.path.join(session_save_path,'norm_data_hist.png'))\n",
    "\n",
    "# make figure (and a place to save it)\n",
    "f_loss = plt.figure()\n",
    "ax_loss = f_loss.add_subplot(1,1,1)\n",
    "\n",
    "for e_idx, epoch in enumerate(range(N_EPOCHS)):\n",
    "    print_progress_bar(epoch,N_EPOCHS,status='epoch:\\t{}'.format(epoch))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # get new train/test splits\n",
    "    # note: add switch to genLoaders to allow for fixed/random sampling\n",
    "    train_loader, test_loader, _, plot_loader = EcogDataloader.genLoaders(dataset, sample_idx, train_frac, test_frac, valid_frac, BATCH_SIZE, rand_samp=RAND_SAMP, plot_seed=plot_seed_idx)\n",
    "\n",
    "#     print('Training Network:')\n",
    "    _, trbl_ = Training.train(model, train_loader, optimizer, criterion, CLIP, TFR)\n",
    "    train_batch_loss.append(trbl_)\n",
    "    train_loss[e_idx] = np.mean(trbl_) # this is the plotted training loss\n",
    "#     print('Testing Network:')\n",
    "    _, tebl_, _ = Training.evaluate(model, test_loader, criterion)\n",
    "    # test_batch_loss.append(tebl_)\n",
    "    test_loss[e_idx] = np.mean(tebl_) # this is the plotted test loss\n",
    "#     print('Running Figure Sequence:')\n",
    "    plot_loss, plbl_, plot_data_list = Training.evaluate(model, plot_loader, criterion, plot_flag=True)\n",
    "    if not (epoch % 10):\n",
    "#         print('Saving estimate plots:')\n",
    "        # save the data for the plotting window in dict form\n",
    "        epoch_plot_path = os.path.join(sequence_plot_path,'epoch{}'.format(epoch))\n",
    "        os.makedirs(epoch_plot_path)\n",
    "        torch.save(model.state_dict(),os.path.join(epoch_plot_path,'model_epoch{}.pt'.format(epoch)))\n",
    "        c_list = ['b','r']\n",
    "        for k in range(len(plot_data_list)):\n",
    "            c_output = c_list[k//n_plot_seed] # blue for training windows, red for testing windows\n",
    "            plot_data_dict = {\n",
    "                'src': plot_data_list[k][0],\n",
    "                'src_dx': plot_data_list[k][1],\n",
    "                'trg': plot_data_list[k][2],\n",
    "                'out': plot_data_list[k][3],\n",
    "                'enc': plot_data_list[k][4],\n",
    "                'dec': plot_data_list[k][5],\n",
    "                'srate': srate_down,\n",
    "                # 'state_dict': model.state_dict(), # putting this in every file is redundant\n",
    "            }\n",
    "            torch.save(plot_data_dict,os.path.join(epoch_plot_path,'data_tuple_epoch{}_window{}.pt'.format(epoch,k)))\n",
    "            # pass data to plotting function for this window\n",
    "            # for plots:\n",
    "            # blue: train\n",
    "            # red: test\n",
    "            # black: real\n",
    "            # green: encoder\n",
    "            # magenta: decoder\n",
    "            f_out, f_enc, f_dec, f_src = Training.eval_plot(plot_data_dict,c_output=c_output)\n",
    "            # save plots in current epoch subdir\n",
    "            f_out.savefig(os.path.join(epoch_plot_path,'output_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "            f_enc.savefig(os.path.join(epoch_plot_path,'encoder_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "            f_dec.savefig(os.path.join(epoch_plot_path,'decoder_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "            f_src.savefig(os.path.join(epoch_plot_path,'source_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "            [plt.close(f) for f in [f_out,f_enc,f_dec,f_src]]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = Util.epoch_time(start_time, end_time)\n",
    "\n",
    "    if test_loss[e_idx] < best_test_loss:\n",
    "        best_test_loss = test_loss[e_idx]\n",
    "        torch.save({ # this needs to be made into a model class method!\n",
    "                'epoch': epoch,\n",
    "                'num_epochs': N_EPOCHS,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_test_loss,\n",
    "                'data_path': data_file_full_path,\n",
    "                'train_frac': train_frac,\n",
    "                'test_frac': test_frac,\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'encoder_length': enc_len,\n",
    "                'decoder_length': dec_len,\n",
    "                }, os.path.join(session_save_path,'model_checkpoint.pt'))\n",
    "\n",
    "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'\\tTrain Loss: {train_loss[e_idx]:.3g}')\n",
    "#     print(f'\\t Test Loss: {test_loss[e_idx]:.3g}')\n",
    "    if e_idx == 0:\n",
    "        ax_loss.plot(e_idx,train_loss[e_idx],'b.',label='train loss')\n",
    "        ax_loss.plot(e_idx,test_loss[e_idx],'r.',label='valid. loss')\n",
    "        ax_loss.legend(loc=0)\n",
    "    else:\n",
    "        ax_loss.plot(e_idx,train_loss[e_idx],'b.')\n",
    "        ax_loss.plot(e_idx,test_loss[e_idx],'r.')\n",
    "    ax_loss.set_ylim(bottom=0,top=1.05*np.concatenate((train_loss,test_loss)).max())\n",
    "    # print the loss curve figure; continuously overwrite (like a fun stock ticker)\n",
    "    f_loss.savefig(os.path.join(session_save_path,'training_progress.png'))\n",
    "    torch.save({'train_loss':train_loss,'test_loss':test_loss,},os.path.join(session_save_path,'training_progress.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
