{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from os import makedirs, chmod\n",
    "import glob\n",
    "import functools\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "import aopy\n",
    "import ecog_is2s\n",
    "\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(data,train_valid_test_frac=(0.8,0.2,0.0)):\n",
    "    # channel mask\n",
    "    ch_var = data.var(axis=0)\n",
    "    ch_var_mean = ch_var.mean()\n",
    "    ch_var_std = ch_var.std()\n",
    "    # ch_idx = np.logical_and(ch_var > ch_var_mean-1.5*ch_var_std, ch_var < ch_var_mean+1.5*ch_var_std)\n",
    "    ch_idx = [28,27,5,24,57,43,8,31,41,37,44,47,59,9,49,20,53,45,25,14,13,40,21,35,39,48,58,52,33,46,56,17,60,30,23,61,15,34,54,51,10,42]\n",
    "    # partition data\n",
    "    n_samples, _ = data.shape\n",
    "    n_ch = sum(ch_idx)\n",
    "    n_train_samples = round(n_samples*train_valid_test_frac[0])\n",
    "    n_valid_samples = round(n_samples*train_valid_test_frac[1])\n",
    "    n_test_samples = round(n_samples*train_valid_test_frac[2])\n",
    "    train_idx = np.arange(0,n_train_samples)\n",
    "    valid_idx = np.arange(n_train_samples,n_train_samples+n_valid_samples)\n",
    "    test_idx = np.arange(n_train_samples+n_valid_samples,n_samples)\n",
    "    train_data = data[train_idx,:][:,ch_idx]\n",
    "    valid_data = data[valid_idx,:][:,ch_idx]\n",
    "    test_data = data[test_idx,:][:,ch_idx]\n",
    "    train_data = train_data - train_data.mean()\n",
    "    valid_data = valid_data - valid_data.mean()\n",
    "    test_data = test_data - test_data.mean()\n",
    "    return train_data, valid_data, test_data, ch_idx\n",
    "\n",
    "def train_AR_model(train_data,model_order):\n",
    "    model = VAR(train_data)\n",
    "    model_fit = model.fit(model_order)\n",
    "    return model_fit\n",
    "\n",
    "def compute_err_metrics(pred,trg,axis=0):\n",
    "    assert pred.shape == trg.shape, \"pred and trg arrays must have equal shape\"\n",
    "    n_sample, n_ch = pred.shape\n",
    "    err = pred-trg\n",
    "    # mean square error\n",
    "    mse = np.mean(err**2,axis=axis)\n",
    "    # root mean square error\n",
    "    rmse = np.sqrt(mse)\n",
    "    # mean absolute error\n",
    "    mae = np.mean(np.abs(err),axis=axis)\n",
    "    # relative prediction error, MSE scaled by target \\sigma\n",
    "    trg_std = trg.std(axis=axis)\n",
    "    rpe = rmse/trg_std\n",
    "    # normalized mae\n",
    "    nmae = mae/np.abs(trg).mean(axis=axis)\n",
    "    # element-wise correlation coefficient\n",
    "    corr = np.diag(np.corrcoef(pred,trg,rowvar=False)[:n_ch,n_ch:])\n",
    "    return mse, rmse, rpe, corr, mae, nmae\n",
    "\n",
    "def compute_prediction_metrics(test_data,model_fit,pred_window_T,bin_T,p_lim=[2.5, 97.5],srate=250):\n",
    "    n_sample, n_ch = test_data.shape\n",
    "    time = np.arange(pred_window_T*srate)/srate\n",
    "    bin_T_left_edge = np.arange(pred_window_T,step=bin_T)\n",
    "    bin_T_right_edge = bin_T_left_edge + bin_T\n",
    "    fit_order = model_fit.k_ar\n",
    "    pred_window_n = int(pred_window_T*srate + fit_order)\n",
    "    n_pred_window = n_sample//pred_window_n\n",
    "    n_time_bin = len(bin_T_left_edge)\n",
    "    mse = np.empty((n_pred_window,n_ch))\n",
    "    mse_all = np.empty((n_pred_window))\n",
    "    mse_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    mse_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    rmse = np.empty((n_pred_window,n_ch))\n",
    "    rmse_all = np.empty((n_pred_window))\n",
    "    rmse_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    rmse_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    mae = np.empty((n_pred_window,n_ch))\n",
    "    mae_all = np.empty((n_pred_window))\n",
    "    mae_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    mae_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    nmae = np.empty((n_pred_window,n_ch))\n",
    "    nmae_all = np.empty((n_pred_window))\n",
    "    nmae_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    nmae_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    rpe = np.empty((n_pred_window,n_ch))\n",
    "    rpe_all = np.empty((n_pred_window))\n",
    "    rpe_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    rpe_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    corr = np.empty((n_pred_window,n_ch))\n",
    "    # corr_all = np.empty((n_pred_window))\n",
    "    corr_bin = np.empty((n_pred_window,n_ch,n_time_bin))\n",
    "    # corr_bin_all = np.empty((n_pred_window,n_time_bin))\n",
    "    for pred_win_idx in tqdm.tqdm(range(n_pred_window)):\n",
    "        window_idx = pred_win_idx*pred_window_n + np.arange(pred_window_n)\n",
    "        data_window = test_data[window_idx,:]\n",
    "        pred = model_fit.forecast(data_window[:fit_order,:],steps=pred_window_n-fit_order)\n",
    "        ## time bins\n",
    "        for tb_idx in range(n_time_bin):\n",
    "            bin_idx = np.logical_and(time >= bin_T_left_edge[tb_idx], time < bin_T_right_edge[tb_idx])\n",
    "            mse_bin[pred_win_idx,:,tb_idx], rmse_bin[pred_win_idx,:,tb_idx], rpe_bin[pred_win_idx,:,tb_idx], corr_bin[pred_win_idx,:,tb_idx], mae_bin[pred_win_idx,:,tb_idx], nmae_bin[pred_win_idx,:,tb_idx] = compute_err_metrics(pred[bin_idx,:],data_window[fit_order:,:][bin_idx,:])\n",
    "            mse_bin_all[pred_win_idx,tb_idx], rmse_bin_all[pred_win_idx,tb_idx], rpe_bin_all[pred_win_idx,tb_idx], _, mae_bin_all[pred_win_idx,tb_idx], nmae_bin_all[pred_win_idx,tb_idx] = compute_err_metrics(pred[bin_idx,:],data_window[fit_order:,:][bin_idx,:],axis=(0,1))\n",
    "        mse[pred_win_idx,:], rmse[pred_win_idx,:], rpe[pred_win_idx,:], corr[pred_win_idx,:], mae[pred_win_idx,:], nmae[pred_win_idx,:] = compute_err_metrics(pred,data_window[fit_order:,:])\n",
    "        mse_all[pred_win_idx], rmse[pred_win_idx], rpe_all[pred_win_idx], _, mae_all[pred_win_idx], nmae_all[pred_win_idx] = compute_err_metrics(pred,data_window[fit_order:,:],axis=(0,1))\n",
    "        trg_fft = np.fft.fft(data_window[fit_order:,:],axis=0)[:int(pred_window_T*srate/2),:]\n",
    "        pred_fft = np.fft.fft(pred,axis=0)[:int(pred_window_T*srate/2),:]\n",
    "        f_fft = np.fft.fftfreq(srate,d=1/srate)[:int(pred_window_T*srate/2)]\n",
    "        # add coherence stats here!\n",
    "    # get stats from sample distributions\n",
    "    stat_dict = {\n",
    "        'mse_mean': mse.mean(axis=0),\n",
    "        'mse_95ci': np.percentile(mse,p_lim,axis=0),\n",
    "        'mse_bin_mean': mse_bin.mean(axis=0),\n",
    "        'mse_bin_95ci': np.percentile(mse_bin,p_lim,axis=0),\n",
    "        'rmse_mean': rmse.mean(axis=0),\n",
    "        'rmse_95ci': np.percentile(rmse,p_lim,axis=0),\n",
    "        'rmse_bin_mean': rmse_bin.mean(axis=0),\n",
    "        'rmse_bin_95ci': np.percentile(rmse_bin,p_lim,axis=0),\n",
    "        'rpe_mean': rpe.mean(axis=0),\n",
    "        'rpe_95ci': np.percentile(rpe,p_lim,axis=0),\n",
    "        'rpe_bin_mean': rpe_bin.mean(axis=0),\n",
    "        'rpe_bin_95ci': np.percentile(rpe_bin,p_lim,axis=0),\n",
    "        'mae_mean': mae.mean(axis=0),\n",
    "        'mae_95ci': np.percentile(mae,p_lim,axis=0),\n",
    "        'mae_bin_mean': mae_bin.mean(axis=0),\n",
    "        'mae_bin_95ci': np.percentile(mae_bin,p_lim,axis=0),\n",
    "        'nmae_mean': nmae.mean(axis=0),\n",
    "        'nmae_95ci': np.percentile(nmae,p_lim,axis=0),\n",
    "        'nmae_bin_mean': nmae_bin.mean(axis=0),\n",
    "        'nmae_bin_95ci': np.percentile(nmae_bin,p_lim,axis=0),\n",
    "        'corr_mean': np.tanh(np.arctanh(corr).mean(axis=0)),\n",
    "        'corr_95ci': np.percentile(corr,p_lim,axis=0),\n",
    "        'corr_bin_mean': np.tanh(np.arctanh(corr_bin).mean(axis=0)),\n",
    "        'corr_bin_95ci': np.percentile(corr_bin,p_lim,axis=0)\n",
    "    }\n",
    "    stat_dict_all = {\n",
    "        'mse_mean': mse_all.mean(axis=0),\n",
    "        'mse_95ci': np.percentile(mse_all,p_lim,axis=0),\n",
    "        'mse_bin_mean': mse_bin_all.mean(axis=0),\n",
    "        'mse_bin_95ci': np.percentile(mse_bin_all,p_lim,axis=0),\n",
    "        'rmse_mean': rmse_all.mean(axis=0),\n",
    "        'rmse_95ci': np.percentile(rmse_all,p_lim,axis=0),\n",
    "        'rmse_bin_mean': rmse_bin_all.mean(axis=0),\n",
    "        'rmse_bin_95ci': np.percentile(rmse_bin_all,p_lim,axis=0),\n",
    "        'rpe_mean': rpe_all.mean(axis=0),\n",
    "        'rpe_95ci': np.percentile(rpe_all,p_lim,axis=0),\n",
    "        'rpe_bin_mean': rpe_bin_all.mean(axis=0),\n",
    "        'rpe_bin_95ci': np.percentile(rpe_bin_all,p_lim,axis=0),\n",
    "        'mae_mean': mae_all.mean(axis=0),\n",
    "        'mae_95ci': np.percentile(mae_all,p_lim,axis=0),\n",
    "        'mae_bin_mean': mae_bin_all.mean(axis=0),\n",
    "        'mae_bin_95ci': np.percentile(mae_bin_all,p_lim,axis=0),\n",
    "        'nmae_mean': nmae_all.mean(axis=0),\n",
    "        'nmae_95ci': np.percentile(nmae_all,p_lim,axis=0),\n",
    "        'nmae_bin_mean': nmae_bin_all.mean(axis=0),\n",
    "        'nmae_bin_95ci': np.percentile(nmae_bin_all,p_lim,axis=0),\n",
    "    }\n",
    "    return stat_dict, stat_dict_all, bin_T_left_edge\n",
    "\n",
    "def create_metric_stat_table(stat_dict):\n",
    "    df = pd.DataFrame(data = {\n",
    "        'mse_mean': [stat_dict['mse_mean']],\n",
    "        'mse_ci_2.5': [stat_dict['mse_95ci'][0,]],\n",
    "        'mse_ci_97.5': [stat_dict['mse_95ci'][1,]],\n",
    "        'rmse_mean': [stat_dict['rmse_mean']],\n",
    "        'rmse_ci_2.5': [stat_dict['rmse_95ci'][0,]],\n",
    "        'rmse_ci_97.5': [stat_dict['rmse_95ci'][1,]],\n",
    "        'rpe_mean': [stat_dict['rpe_mean']],\n",
    "        'rpe_ci_2.5': [stat_dict['rpe_95ci'][0,]],\n",
    "        'rpe_ci_97.5': [stat_dict['rpe_95ci'][1,]],\n",
    "        'mae_mean': [stat_dict['mae_mean']],\n",
    "        'mae_ci_2.5': [stat_dict['mae_95ci'][0,]],\n",
    "        'mae_ci_97.5': [stat_dict['mae_95ci'][1,]],\n",
    "        'nmae_mean': [stat_dict['nmae_mean']],\n",
    "        'nmae_ci_2.5': [stat_dict['nmae_95ci'][0,]],\n",
    "        'nmae_ci_97.5': [stat_dict['nmae_95ci'][1,]],\n",
    "        'corr_mean': [stat_dict['corr_mean']],\n",
    "        'corr_ci_2.5': [stat_dict['corr_95ci'][0,]],\n",
    "        'corr_ci_97.5': [stat_dict['corr_95ci'][1,]],\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_metric_all_stat_table(stat_dict):\n",
    "    df = pd.DataFrame(data = {\n",
    "        'mse_mean': [stat_dict['mse_mean']],\n",
    "        'mse_ci_2.5': [stat_dict['mse_95ci'][0,]],\n",
    "        'mse_ci_97.5': [stat_dict['mse_95ci'][1,]],\n",
    "        'rmse_mean': [stat_dict['rmse_mean']],\n",
    "        'rmse_ci_2.5': [stat_dict['rmse_95ci'][0,]],\n",
    "        'rmse_ci_97.5': [stat_dict['rmse_95ci'][1,]],\n",
    "        'rpe_mean': [stat_dict['rpe_mean']],\n",
    "        'rpe_ci_2.5': [stat_dict['rpe_95ci'][0,]],\n",
    "        'rpe_ci_97.5': [stat_dict['rpe_95ci'][1,]],\n",
    "        'mae_mean': [stat_dict['mae_mean']],\n",
    "        'mae_ci_2.5': [stat_dict['mae_95ci'][0,]],\n",
    "        'mae_ci_97.5': [stat_dict['mae_95ci'][1,]],\n",
    "        'nmae_mean': [stat_dict['nmae_mean']],\n",
    "        'nmae_ci_2.5': [stat_dict['nmae_95ci'][0,]],\n",
    "        'nmae_ci_97.5': [stat_dict['nmae_95ci'][1,]],\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_metric_bin_stat_table(stat_dict):\n",
    "    df = pd.DataFrame(data = {\n",
    "        'mse_bin_mean': [stat_dict['mse_bin_mean']],\n",
    "        'mse_ci_2.5': [stat_dict['mse_bin_95ci'][0,]],\n",
    "        'mse_ci_97.5': [stat_dict['mse_bin_95ci'][1,]],\n",
    "        'rmse_bin_mean': [stat_dict['rmse_bin_mean']],\n",
    "        'rmse_ci_2.5': [stat_dict['rmse_bin_95ci'][0,]],\n",
    "        'rmse_ci_97.5': [stat_dict['rmse_bin_95ci'][1,]],\n",
    "        'rpe_bin_mean': [stat_dict['rpe_bin_mean']],\n",
    "        'rpe_ci_2.5': [stat_dict['rpe_bin_95ci'][0,]],\n",
    "        'rpe_ci_97.5': [stat_dict['rpe_bin_95ci'][1,]],\n",
    "        'mae_bin_mean': [stat_dict['mae_bin_mean']],\n",
    "        'mae_ci_2.5': [stat_dict['mae_bin_95ci'][0,]],\n",
    "        'mae_ci_97.5': [stat_dict['mae_bin_95ci'][1,]],\n",
    "        'nmae_bin_mean': [stat_dict['nmae_bin_mean']],\n",
    "        'nmae_ci_2.5': [stat_dict['nmae_bin_95ci'][0,]],\n",
    "        'nmae_ci_97.5': [stat_dict['nmae_bin_95ci'][1,]],\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single file data array\n",
    "# file list to dataset\n",
    "data_path_root = 'C:\\\\Users\\\\mickey\\\\aoLab\\\\Data\\\\WirelessData\\\\Goose_Multiscale_M1'\n",
    "data_path_day = path.join(data_path_root,'18032*')\n",
    "data_file_list = glob.glob(path.join(data_path_day,'0[0-9]*\\\\*ECOG*clfp_ds250_fl0u10.dat'))\n",
    "# data_file_list = glob.glob(path.join(data_path_day,'0[0-9]*\\\\*ECOG*clfp_ds250.dat'))\n",
    "device = 'cpu'\n",
    "print('mounting to device: {}'.format(device))\n",
    "print(f'files found:\\t{len(data_file_list)}')\n",
    "print(f'files: {data_file_list}')\n",
    "datafile_list = [aopy.data.DataFile(df) for df in data_file_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop across all files to:\n",
    "#   - train AR model\n",
    "#   - compute error metrics\n",
    "#   - create error table\n",
    "# concatenate all error tables\n",
    "# save table\n",
    "\n",
    "mask_buffer_T = 60\n",
    "model_order = 10\n",
    "pred_window_T = 1\n",
    "bin_T = 0.1\n",
    "stat_df_list = []\n",
    "stat_all_df_list = []\n",
    "stat_bin_df_list = []\n",
    "file_used = np.zeros(len(datafile_list),dtype=bool)\n",
    "file_path_used = []\n",
    "\n",
    "date_str = datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d%H%M%S')\n",
    "analysis_path = f'D:\\\\Users\\\\mickey\\\\Data\\\\analysis\\\\prediction_p{model_order}_{pred_window_T}s_{date_str}'\n",
    "if not path.exists(analysis_path):\n",
    "    os.makedirs(analysis_path)\n",
    "os.makedirs(path.join(analysis_path,'models'))\n",
    "\n",
    "for df_idx, df in enumerate(datafile_list):\n",
    "    print(f'({df_idx+1}/{len(datafile_list)}) file {df.data_file_path}')\n",
    "    data = df.read().T\n",
    "    mask = df.data_mask\n",
    "    srate = df.srate\n",
    "    if mask.mean() < 0.8:\n",
    "        # try:\n",
    "            file_used[df_idx] = True\n",
    "            # widen masked regions - no chances\n",
    "            print(f'masking data...')\n",
    "            mask_buffer_n = mask_buffer_T*srate\n",
    "            fill_mask = sp.signal.convolve(mask,np.ones(mask_buffer_n,dtype=bool),mode='same')\n",
    "            data = data[~fill_mask,:]\n",
    "            train_data, valid_data, test_data, ch_idx = partition_data(data)\n",
    "            # train the linear model\n",
    "            print('fitting model...')\n",
    "            model_fit = train_AR_model(train_data,model_order)\n",
    "            # compute validation metrics\n",
    "            print('computing metrics:')\n",
    "            stat_dict, stat_dict_all, bin_T_left_edge = compute_prediction_metrics(valid_data,model_fit,pred_window_T,bin_T,srate=srate)\n",
    "            # convert to tables\n",
    "            _stat_df = create_metric_stat_table(stat_dict)\n",
    "            _stat_all_df = create_metric_all_stat_table(stat_dict_all)\n",
    "            _stat_bin_df = create_metric_bin_stat_table(stat_dict_all)\n",
    "            stat_df_list.append(_stat_df)\n",
    "            stat_all_df_list.append(_stat_all_df)\n",
    "            stat_bin_df_list.append(_stat_bin_df)\n",
    "            # save the AR model\n",
    "            model_fit.save(path.join(analysis_path,'models',f'model_{str(df_idx)}'))\n",
    "            file_path_used.append(df.data_file_path)\n",
    "        # except:\n",
    "        #     continue\n",
    "stat_df = pd.concat(stat_df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df['file_path'] = file_path_used\n",
    "stat_all_df = pd.concat(stat_all_df_list)\n",
    "stat_all_df['file_path'] = file_path_used\n",
    "stat_bin_df = pd.concat(stat_bin_df_list)\n",
    "stat_bin_df['file_path'] = file_path_used\n",
    "# stat_bin_df['bin_t'] = repmat(bin_T_left_edge,1,file_used.sum())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save performance metric table for this analysis\n",
    "stat_df.to_csv(path.join(analysis_path,'prediction_metric_stats.csv'))\n",
    "stat_all_df.to_csv(path.join(analysis_path,'prediction_metric_all_stats.csv'))\n",
    "stat_bin_df.to_csv(path.join(analysis_path,'prediction_metric_bin_stats.csv'))\n",
    "param_dict = {\n",
    "    'bin_t': bin_T_left_edge,\n",
    "    'pred_t': pred_window_T,\n",
    "    'bin_t': bin_T,\n",
    "    'model_p': model_order\n",
    "}\n",
    "np.savez(path.join(analysis_path,'param.npz'),param_dict)"
   ]
  }
 ]
}