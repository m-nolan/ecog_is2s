{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECoG Spectrogram Forecasting with Sequence-to-Sequence (seq2seq) RNN models.\n",
    "\n",
    "## Starting small: 1 channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from aopy import datareader, datafilter\n",
    "from ecog_is2s import EcogDataloader, Training, Encoder, Decoder, Seq2Seq, Util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# import progressbar as pb\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modules that aren't done yet\n",
    "# sys.path.append('/Users/mickey/aoLab/code/analyze/')\n",
    "sys.path.append('/home/mickey/analyze/')\n",
    "from tfspec import tfspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell is used to parse command line arguments. These arguments assign values to the network and training parameters. This functionality has been replaced with more hard-coded constants in the current notebook. From an organizational standpoint, that's not the worst thing - it's given me a good opportunity to collect all of the constants together into one single code block @ the top of the notebook. They were pretty scattershot before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grab input arguments\n",
    "# parser = argparse.ArgumentParser('Trains a seq2seq network on a section of example NHP PMC ECoG data.',add_help=True)\n",
    "# parser.add_argument('--encoder-depth', metavar='el', type=int, default=10, help='Sequence depth of the encoder network')\n",
    "# parser.add_argument('--decoder-depth', metavar='dl', type=int, default=1, help='Sequence depth of the decoder network')\n",
    "# parser.add_argument('--batch-size', metavar='b', type=int, default=1, help='Data batch size')\n",
    "# parser.add_argument('--num-epochs', metavar='n', type=int, default=1, help='Number of optimization epochs')\n",
    "# parser.add_argument('--num-layers', metavar='nl', type=int, default=1, help='Number of layers in each RNN block')\n",
    "\n",
    "# args = parser.parse_args() # this bad boy has all the values packed into it. Nice!\n",
    "# print(args.encoder_depth,args.decoder_depth)\n",
    "\n",
    "# print(args.encoder_depth,args.decoder_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "T_MINUTES = 60\n",
    "ENCODER_DEPTH = 30\n",
    "DECODER_DEPTH = 30\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 1000\n",
    "N_EPOCHS = NUM_EPOCHS\n",
    "RNG_SEED = 5050\n",
    "INPUT_SEQ_LEN = ENCODER_DEPTH\n",
    "OUTPUT_SEQ_LEN = DECODER_DEPTH\n",
    "N_CH_USE = 1\n",
    "N_LAYER = 1\n",
    "DROPOUT = np.float32(0.3)\n",
    "ENC_DROPOUT = np.float32(DROPOUT)\n",
    "DEC_DROPOUT = np.float32(DROPOUT)\n",
    "LEARN_RATE = 0.001 # default ADAM: 0.001\n",
    "LOSS_OBJ = 'MSE' #L1, L2, see training.py:ECOGLoss()\n",
    "WEIGHT_RANGE = (-0.2,0.2) # ignore for now; not sure how to worm this through\n",
    "train_frac = 0.5\n",
    "test_frac = 0.5\n",
    "valid_frac = 0.0\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# N_EPOCHS = args.num_epochs\n",
    "CLIP = 1. # this the maximum norm of the whole parameter gradient.\n",
    "TFR = 0.0 # no teacher forcing! Anything it's learning is all on its own\n",
    "RAND_SAMP = False\n",
    "weight_reg = 0.#0.0003\n",
    "enc_len = ENCODER_DEPTH\n",
    "dec_len = DECODER_DEPTH\n",
    "seq_len = ENCODER_DEPTH+DECODER_DEPTH # use ten time points to predict the next time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed RNG for pytorch/np\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "torch.cuda.manual_seed(RNG_SEED)\n",
    "torch.backends.cudnn.deterministic = True # enforces deterministic algorithm use -> reproducibility. Remove for production code. You don't do production code. Don't remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device - CUDA if you've got it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('mounting to device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "platform_name = sys.platform\n",
    "if platform_name == 'darwin':\n",
    "    # local machine\n",
    "    data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.lfp.dat'\n",
    "    mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "    model_save_dir_path = '/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq/'\n",
    "elif platform_name == 'linux2':\n",
    "    # HYAK, baby!\n",
    "    data_file_full_path = '/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = \"/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "elif platform_name == 'linux':\n",
    "    # google cloud, don't fail me now\n",
    "    data_file_full_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.mask.pkl'\n",
    "    model_save_dir_path = '/home/mickey/models/pyt/seq2seq/'\n",
    "\n",
    "# make sure the output directory actually exists\n",
    "if not os.path.exists(model_save_dir_path):\n",
    "    os.makedirs(model_save_dir_path)\n",
    "\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in = data_param['srate']\n",
    "num_ch = data_param['num_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask data array, remove obvious outliers\n",
    "# data_in[:,np.logical_or(data_mask[\"hf\"],data_mask[\"sat\"])] = 0.\n",
    "\n",
    "# downsample data\n",
    "srate_down = 250\n",
    "srate = srate_in\n",
    "\n",
    "# grab local time segment\n",
    "total_len_T = T_MINUTES*60\n",
    "total_len_n = total_len_T*srate_in\n",
    "# data_idx = data_in.shape[1]//2 + np.arange(total_len_n)\n",
    "print('Downsampling data from {0} to {1}'.format(srate_in,srate_down))\n",
    "### note: this breaks mask indexing, if you plan to do that later.\n",
    "data_in = np.float32(sp.signal.decimate(data_in[:N_CH_USE,:],srate_in//srate_down,axis=-1))\n",
    "data_in = np.float32(data_in[:N_CH_USE,:])\n",
    "data_in = data_in*1e-3 # convert to mV\n",
    "print('Data Size:\\t{}\\n'.format(data_in.shape))\n",
    "\n",
    "# # filter dead channels\n",
    "# ch_rms = np.std(data_in,axis=-1)\n",
    "# ch_m = np.mean(ch_rms)\n",
    "# ch_low_lim = ch_m - 2*np.std(ch_rms)\n",
    "# ch_up_lim = ch_m + 2*np.std(ch_rms)\n",
    "# ch_idx = np.logical_and(ch_rms > ch_low_lim, ch_rms < ch_up_lim)\n",
    "# ch_list = np.arange(num_ch)[ch_idx]\n",
    "# num_ch_down = len(ch_list)\n",
    "# print('Filtering Channels:')\n",
    "# print('Num. ch. used:\\t{}'.format(num_ch_down))\n",
    "# print('Ch. kept:\\t{}\\n'.format(ch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute data spectrogram\n",
    "tapers = [4,1];\n",
    "dn = 0.5;\n",
    "# T = 60\n",
    "sgram, f_sgram, ti, err = tfspec(data_in, tapers=tapers, sampling=srate_down, dn=dn,fk=(0,120), contflag=False);\n",
    "t_sgram = ti/srate_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter bad regions by spectrogram power\n",
    "print('Detecting bad spectrogram regions')\n",
    "db_power = np.clip(10*np.ma.log10(sgram).mean(axis=-1)[0,:],a_min=-50,a_max=np.inf)\n",
    "db_power_mean = db_power.mean()\n",
    "db_power_std = db_power.std()\n",
    "std_k = 0.25\n",
    "bad_sgram_idx = (db_power.data < db_power_mean - 3*std_k*db_power_std) | \\\n",
    "                (db_power.data > db_power_mean + std_k*db_power_std) | \\\n",
    "                (db_power.mask)\n",
    "print('{}% retained'.format(100-bad_sgram_idx.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(db_power,100)\n",
    "plt.axvline(db_power_mean - 3*std_k*db_power_std)\n",
    "plt.axvline(db_power_mean + std_k*db_power_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for spectrogram plotting; forgive the misnomer\n",
    "def plot_log_psd(ax,psd,x,y,clim,cmap='viridis',cb_label='dB'):\n",
    "    # create norm from clim\n",
    "    norm = plt.Normalize(vmin=clim[0],vmax=clim[1])\n",
    "    # get axes extent from x, y\n",
    "    extent = (x.min(), x.max(), y.min(), y.max())\n",
    "    # plot the figure\n",
    "    im_h = ax.imshow(10*np.log10(psd),extent=extent,norm=norm,origin='bottom',aspect='auto')\n",
    "    # add a colorbar\n",
    "    cb_h = plt.colorbar(im_h, ax=ax, label=cb_label, pad=0.01)\n",
    "    \n",
    "    return im_h, cb_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for spectrogram plotting; forgive the misnomer\n",
    "def plot_psd(ax,psd,x,y,clim,cmap='viridis',cb_label='power'):\n",
    "    # create norm from clim\n",
    "    norm = plt.Normalize(vmin=clim[0],vmax=clim[1])\n",
    "    # get axes extent from x, y\n",
    "    extent = (x.min(), x.max(), y.min(), y.max())\n",
    "    # plot the figure\n",
    "    im_h = ax.imshow(psd,extent=extent,norm=norm,origin='bottom',aspect='auto',cmap=cmap)\n",
    "    # add a colorbar\n",
    "    cb_h = plt.colorbar(im_h, ax=ax, label=cb_label, pad=0.01)\n",
    "    \n",
    "    return im_h, cb_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_col = 6\n",
    "# f,ax = plt.subplots(n_col,1,figsize=(25,10),dpi=100)\n",
    "# n_win = sgram.shape[1]\n",
    "# n_win_per_row = n_win//n_col\n",
    "# clim = (-25,-5)\n",
    "# for k in range(n_col):\n",
    "#     plot_idx = np.arange(k*n_win_per_row,(k+1)*n_win_per_row)\n",
    "#     plot_log_psd(ax[k],sgram[:,plot_idx,].mean(axis=0).T,t_sgram[plot_idx],f_sgram,clim,cb_label='db$\\mu$')\n",
    "#     ax[k].set_ylim(0,60)\n",
    "# ax[-1].set_ylabel('freq. (Hz)')\n",
    "# ax[-1].set_xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset/tensor from the spectrogram data\n",
    "Spectrogram data is a $n_{ch}\\times n_{win}\\times n_{freq}$ array, so creating a pytorch tensor, dataset and loader from a spectrogram is not very complicated. However, doing that in a way that plays well nicely with recurrent neural network models is less straightforward, primarily because of the variability in frequency band power across the spectrogram and the requirement of normalized data inputs.\n",
    "\n",
    "How should I do this? It's an open question for me, but some paper probably has the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "#create data tensor\n",
    "print('Normalizing data, converting to tensor:')\n",
    "data_rail = np.max(np.abs(data_in.reshape(-1)))\n",
    "# normalization = 'zscore'\n",
    "# normalization = 'tanh'\n",
    "# if normalization is 'max':\n",
    "#     data_tensor = torch.from_numpy(data_in.view().T/data_rail)\n",
    "# elif normalization is 'zscore':\n",
    "#     # for nominally gaussian data distributions, this will get ~99% of data points in (-1, 1)\n",
    "#     data_tensor = torch.from_numpy(sp.stats.zscore(data_in.view().T)/5)\n",
    "# elif normalization is 'tanh':\n",
    "#     data_tensor = torch.from_numpy(np.tanh(sp.stats.zscore(data_in.view().T)/3))\n",
    "# print('Data tensor shape: {}\\n'.format(data_tensor.shape))\n",
    "N_FREQ_BINS = -1\n",
    "n_sgram_win = sgram.shape[-1]\n",
    "freq_range = (1, 80)\n",
    "freq_idx = (f_sgram >= freq_range[0]) & (f_sgram <= freq_range[1])\n",
    "mean_sgram = np.log10(sgram[0,~bad_sgram_idx,:]).mean(axis=0)\n",
    "log_sgram_ceil = np.log10(sgram[0,~bad_sgram_idx,:])[:,freq_idx].max()\n",
    "log_sgram_floor = np.log10(sgram[0,~bad_sgram_idx,:])[:,freq_idx].min()\n",
    "log_sgram_mid = (log_sgram_ceil + log_sgram_floor)/2\n",
    "log_sgram_range = log_sgram_ceil - log_sgram_floor\n",
    "# PCA_REDUCE = False\n",
    "NORMALIZE_DATA = True\n",
    "t_sgram = ti/srate_down\n",
    "# t_sgram_use = (t_sgram > 30*60) & (t_sgram <= 90*60)\n",
    "t_sgram_use = np.ones(t_sgram.shape,dtype=bool)\n",
    "\n",
    "if NORMALIZE_DATA:\n",
    "#     data_array = ((np.log10(sgram[0,])-log_sgram_mid)/log_sgram_range*1.8)[:,freq_idx]\n",
    "    mean_psd = sgram.mean(axis=(0,1))\n",
    "    data_array = sp.stats.zscore(np.log10(sgram[:,t_sgram_use,:]),axis=(0,1))[0,]\n",
    "else:\n",
    "    data_array = np.log10(sgram[0,:,freq_idx]).T\n",
    "# if PCA_REDUCE:\n",
    "#     k = 6\n",
    "#     data_cov = np.cov(data_array[np.logical_not(bad_sgram_idx),:].T)\n",
    "#     data_cov_eigval, data_cov_eigvec = np.linalg.eig(data_cov)\n",
    "#     data_tensor = torch.from_numpy(data_array.dot(data_cov_eigvec[:,:k]))\n",
    "data_tensor = torch.from_numpy(np.float32(data_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_col = 3\n",
    "# f,ax = plt.subplots(n_col,1,figsize=(25,10),dpi=100)\n",
    "# n_win = data_tensor.shape[0]\n",
    "# n_win_per_row = n_win//n_col\n",
    "# clim = (-1, 1)\n",
    "# for k in range(n_col):\n",
    "#     plot_idx = np.arange(k*n_win_per_row,(k+1)*n_win_per_row)\n",
    "#     plot_psd(ax[k],data_tensor[plot_idx,].T,t_sgram[plot_idx]/60+60,f_sgram,clim,cb_label='db$\\mu$',cmap='coolwarm')\n",
    "#     ax[k].set_ylim(0,60)\n",
    "# ax[-1].set_ylabel('freq. (Hz)')\n",
    "# ax[-1].set_xlabel('time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset object\n",
    "print('Creating EcogDataloader dataset object:')\n",
    "if device == 'cuda:0':\n",
    "    data_tensor.cuda()\n",
    "dataset_diff = EcogDataloader.EcogDataset(data_tensor,device,enc_len,dec_len,\n",
    "                                     transform=EcogDataloader.add_signal_diff(srate=1/dn,device=device))\n",
    "dataset_nodiff = EcogDataloader.EcogDataset(data_tensor,device,enc_len,dec_len,transform=None)\n",
    "num_ch_down = dataset_nodiff.n_ch\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_all = np.arange(dataset_nodiff.data.shape[0])\n",
    "idx_step = DECODER_DEPTH\n",
    "sample_idx = idx_all[:-seq_len:idx_step]\n",
    "n_win = sample_idx.shape[0]\n",
    "idx_cut = np.zeros((n_win),dtype=bool)\n",
    "for k in range(n_win):\n",
    "    idx_cut[k] = np.any(bad_sgram_idx[sample_idx[k]:(sample_idx[k]+seq_len)])\n",
    "sample_idx = sample_idx[np.logical_not(idx_cut)]\n",
    "# plot samples\n",
    "n_plot_seed = 1\n",
    "n_plot_step = 4*seq_len\n",
    "plot_seed_idx = np.arange(0,n_plot_seed*n_plot_step,n_plot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple progressbar, not tied to the iterator\n",
    "def print_progress_bar(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of parameter tuples\n",
    "n_hidden_list = [2**10,2**11]\n",
    "n_layer_list = [1,2,3]\n",
    "dropout_list = np.array([0.,0.3],dtype=float)\n",
    "bidir_list = [True, False]\n",
    "use_diff_list = [True, False]\n",
    "param_tuple_list = product(n_hidden_list,n_layer_list,dropout_list,bidir_list,use_diff_list)\n",
    "LEARN_RATE = 0.001\n",
    "min_lr = 0.00005\n",
    "decay_scale = 0.2\n",
    "stop_lag = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep across parameterizations: create and train models on z-scored sgram data\n",
    "n_win, n_freq = dataset_nodiff.data.shape\n",
    "INPUT_DIM = n_freq\n",
    "OUTPUT_DIM = n_freq\n",
    "\n",
    "time_str = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "sweep_output_path = os.path.join(model_save_dir_path,'enc{}_dec{}_nep{}_hps_{}'.format(enc_len,dec_len,N_EPOCHS,time_str))\n",
    "\n",
    "for param_idx, (HID_DIM, N_LAYER, DROPOUT, bidirectional, use_diff) in enumerate(param_tuple_list):\n",
    "    # make network model\n",
    "    model = Seq2Seq.Seq2Seq_GRU(INPUT_DIM, HID_DIM, N_LAYER, ENCODER_DEPTH, DECODER_DEPTH, \n",
    "                                device, dropout=DROPOUT,use_diff=use_diff,bidirectional=bidirectional).to(device)\n",
    "    model.apply(Util.init_weights) # initialize the model each time.\n",
    "\n",
    "    print(f'The model has {Util.count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#     criterion = nn.L1Loss(reduction='mean')\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=LEARN_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=decay_scale, min_lr=min_lr)\n",
    "    \n",
    "    print(model)\n",
    "    \n",
    "    # train model and save outputs\n",
    "    # train/test!\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    train_loss = np.zeros(N_EPOCHS)\n",
    "    train_batch_loss = []\n",
    "    test_loss = np.zeros(N_EPOCHS)\n",
    "    test_batch_loss = []\n",
    "    print_train_loss = np.Inf\n",
    "    print_test_loss = np.Inf\n",
    "    \n",
    "    learning_rate_history = np.zeros(N_EPOCHS)\n",
    "\n",
    "    # create training session directory\n",
    "    time_str = Util.time_str() # I may do well to pack this into util\n",
    "    session_save_path = os.path.join(sweep_output_path,'hid{}_nl{}_d{:0.1f}_diff{}_bi{}'.format(HID_DIM, N_LAYER, DROPOUT,use_diff,bidirectional))\n",
    "    sequence_plot_path = os.path.join(session_save_path,'example_sequence_figs')\n",
    "    os.makedirs(session_save_path) # no need to check; there's no way it exists yet.\n",
    "    os.makedirs(sequence_plot_path)\n",
    "    print('saving session data to:\\t{}'.format(session_save_path))\n",
    "    # save a histogram of the data distribution; allowing you to check\n",
    "    f,ax = plt.subplots(1,1,figsize=(6,4))\n",
    "    ax.hist(dataset_nodiff.data.reshape(-1),100,density=True)\n",
    "    f.savefig(os.path.join(session_save_path,'norm_data_hist.png'))\n",
    "\n",
    "    # make figure (and a place to save it)\n",
    "    f_loss = plt.figure()\n",
    "    ax_loss = f_loss.add_subplot(1,1,1)\n",
    "\n",
    "    for e_idx, epoch in enumerate(range(N_EPOCHS)):\n",
    "        print_progress_bar(epoch,N_EPOCHS,status='epoch: {}\\ttrain loss: {:0.3f}\\tvalid loss:{:0.3f}'.format(epoch,print_train_loss,print_test_loss))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # get new train/test splits\n",
    "        # note: add switch to genLoaders to allow for fixed/random sampling\n",
    "        if use_diff:\n",
    "            dataset_use = dataset_diff\n",
    "        else:\n",
    "            dataset_use = dataset_nodiff\n",
    "        train_loader, test_loader, _, plot_loader = EcogDataloader.genLoaders(dataset_use, sample_idx, train_frac\n",
    "                                                                              , test_frac, valid_frac, BATCH_SIZE\n",
    "                                                                              , rand_samp=RAND_SAMP, plot_seed=plot_seed_idx)\n",
    "        _, trbl_ = model.train_iter(train_loader, optimizer, criterion, CLIP, TFR)\n",
    "        train_batch_loss.append(trbl_)\n",
    "        train_loss[e_idx] = np.mean(trbl_) # this is the plotted training loss\n",
    "        _, tebl_ = model.eval_iter(test_loader, criterion)\n",
    "        test_loss[e_idx] = np.mean(tebl_) # this is the plotted test loss\n",
    "        learning_rate_history[e_idx] = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(test_loss[e_idx])\n",
    "        if not (epoch % 10):\n",
    "            # save the data for the plotting window in dict form\n",
    "            epoch_plot_path = os.path.join(sequence_plot_path,'epoch{}'.format(epoch))\n",
    "            os.makedirs(epoch_plot_path)\n",
    "            torch.save(model.state_dict(),os.path.join(epoch_plot_path,'model_epoch{}.pt'.format(epoch)))\n",
    "            c_list = ['b','r']\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = Util.epoch_time(start_time, end_time)\n",
    "\n",
    "        if test_loss[e_idx] < best_test_loss:\n",
    "            best_test_loss = test_loss[e_idx]\n",
    "            torch.save({ # this needs to be made into a model class method!\n",
    "                    'epoch': epoch,\n",
    "                    'num_epochs': N_EPOCHS,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_test_loss,\n",
    "                    'data_path': data_file_full_path,\n",
    "                    'train_frac': train_frac,\n",
    "                    'test_frac': test_frac,\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'encoder_length': enc_len,\n",
    "                    'decoder_length': dec_len,\n",
    "                    }, os.path.join(session_save_path,'model_checkpoint.pt'))\n",
    "            \n",
    "        if e_idx == 0:\n",
    "            ax_loss.plot(e_idx,train_loss[e_idx],'b.',label='train loss')\n",
    "            ax_loss.plot(e_idx,test_loss[e_idx],'r.',label='valid. loss')\n",
    "            ax_loss.legend(loc=0)\n",
    "        else:\n",
    "            ax_loss.plot(e_idx,train_loss[e_idx],'b.')\n",
    "            ax_loss.plot(e_idx,test_loss[e_idx],'r.')\n",
    "            \n",
    "        ax_loss.set_ylim(bottom=0,top=1.05*np.concatenate((train_loss,test_loss)).max())\n",
    "        # print the loss curve figure; continuously overwrite (like a fun stock ticker)\n",
    "        f_loss.savefig(os.path.join(session_save_path,'training_progress.png'))\n",
    "        torch.save({'train_loss':train_loss,'test_loss':test_loss,},os.path.join(session_save_path,'training_progress.pt'))\n",
    "        print_train_loss = train_loss[e_idx]\n",
    "        print_test_loss = test_loss[e_idx]\n",
    "        \n",
    "        # see if losses are not moving\n",
    "        if e_idx >= stop_lag:\n",
    "            mean_train_loss_change_ratio = np.abs(np.mean((train_loss[e_idx] - train_loss[(e_idx-stop_lag):e_idx])/train_loss[e_idx-stop_lag]))\n",
    "            mean_test_loss_change_ratio = np.abs(np.mean((test_loss[e_idx] - test_loss[(e_idx-stop_lag):e_idx])/test_loss[e_idx-stop_lag]))\n",
    "            if mean_train_loss_change_ratio < 0.05 and mean_test_loss_change_ratio < 0.05 and test_loss[e_idx] < train_loss[0]/2:\n",
    "                break\n",
    "    \n",
    "    # replot with log-y scale \n",
    "    f_log_loss, ax = plt.subplots(2,1,figsize=(5,6),dpi=100,sharex=True)\n",
    "    ax[0].plot(np.arange(epoch),train_loss[:epoch],'b.',label='train loss')\n",
    "    ax[0].plot(np.arange(epoch),test_loss[:epoch],'r.',label='test loss')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[0].set_ylabel('MSE Loss')\n",
    "    ax[0].set_title('Spectrogram Prediction Loss')\n",
    "    ax[0].legend(loc=0)\n",
    "    ax[1].plot(np.arange(epoch),learning_rate_history[:epoch])\n",
    "    ax[1].set_xlabel('epochs/iters')\n",
    "    ax[1].set_ylabel('Learning Rate')\n",
    "    ax[1].set_title('Adaptive Learning Rate')\n",
    "    f_log_loss.savefig(os.path.join(session_save_path,'training_progress_logscale.png'))\n",
    "    plt.close(f_loss)\n",
    "    plt.close(f_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample\n",
    "src, trg = dataset_nodiff.__getitem__(sample_idx[50])\n",
    "src = src.unsqueeze(0)\n",
    "trg = trg.unsqueeze(0)\n",
    "pred,enc,dec = model(src,trg)\n",
    "# plot comparison\n",
    "f_compare,ax = plt.subplots(1,3,figsize=(12,3))\n",
    "asdf = ax[0].imshow(trg[0,].cpu().detach().T,clim=(-.75,0.5),aspect='auto',origin='lower',extent=(0,30,freq_range[0],freq_range[1]));\n",
    "# ax[0].set_yticks(np.arange(0,700,100))\n",
    "# ax[0].set_yticklabels(f_sgram[freq_idx][0:-1:100])\n",
    "ax[0].set_title('Target')\n",
    "asdf = ax[1].imshow(pred[0,].cpu().detach().T,clim=(-.75,0.5),aspect='auto',origin='lower',extent=(0,30,freq_range[0],freq_range[1]));\n",
    "# ax[1].set_yticks(np.arange(0,700,100))\n",
    "# ax[1].set_yticklabels(f_sgram[freq_idx][0:-1:100])\n",
    "ax[1].set_title('Prediction')\n",
    "asdf = ax[2].imshow(trg[0,].cpu().detach().T - pred[0,].cpu().detach().T,clim=(-1.0,1.0),aspect='auto',origin='lower',extent=(0,30,freq_range[0],freq_range[1]),cmap='coolwarm');\n",
    "cbax = f_compare.add_axes([0.95, 0.1, 0.015, 0.8])\n",
    "plt.colorbar(asdf,cax=cbax)\n",
    "ax[2].set_title('Difference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
