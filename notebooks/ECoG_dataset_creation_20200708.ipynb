{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a pytorch dataset from Wireless ECoG data\n",
    "Michael Nolan\n",
    "2020.07.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "# from skimage import io, transform\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WirelessEcogDataset(Dataset):\n",
    "    # Wireless ECoG Dataset\n",
    "    # Pesaran lab data\n",
    "    # AOLab\n",
    "    # Michael Nolan\n",
    "    # 2020.07.08\n",
    "    \n",
    "    def __init__( self, ecog_file, src_len, trg_len, step_len, ch_idx = None):\n",
    "        ## parse file\n",
    "        data_file = os.path.basename(ecog_file)\n",
    "        data_file_kern = os.path.splitext(data_file)[0]\n",
    "        rec_id, microdrive_name, rec_type = data_file_kern.split('.')\n",
    "        data_path = os.path.dirname(ecog_file)\n",
    "        # read experiment file\n",
    "        exp_file = os.path.join(data_path,rec_id + \".experiment.json\")\n",
    "        with open(exp_file,'r') as f:\n",
    "            exp_dict = json.load(f)\n",
    "        # get microdrive parameters\n",
    "        microdrive_name_list = [md['name'] for md in exp_dict['hardware']['microdrive']]\n",
    "        microdrive_idx = [md_idx for md_idx, md in enumerate(microdrive_name_list) if microdrive_name == md][0]\n",
    "        microdrive_dict = exp_dict['hardware']['microdrive'][microdrive_idx]\n",
    "        n_ch = len(microdrive_dict['electrodes'])\n",
    "        if not ch_idx:\n",
    "            ch_idx = np.arange(n_ch)\n",
    "        # get srate\n",
    "        if rec_type == 'raw':\n",
    "            srate = experiment['hardware']['acquisition']['samplingrate']\n",
    "            data_type = np.ushort\n",
    "        elif rec_type == 'lfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        elif rec_type == 'clfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        # read mask\n",
    "        ecog_mask_file = os.path.join(data_path,data_file_kern + \".mask.pkl\")\n",
    "        with open(ecog_mask_file,\"rb\") as mask_f:\n",
    "            mask = pkl.load(mask_f)\n",
    "        mask = mask[\"hf\"] | mask[\"sat\"]\n",
    "        # get params\n",
    "        n_samp = len(mask)\n",
    "        # create sampling index - src_len+trg_len length segments that don't include a masked sample\n",
    "        _sample_idx = np.arange(n_samp-(src_len+trg_len),step=step_len)\n",
    "        _use_sample_idx = np.zeros((len(_sample_idx)),dtype=bool) # all false\n",
    "        for sidx in range(len(_sample_idx)):\n",
    "            _use_sample_idx[sidx] = ~np.any(mask[_sample_idx[sidx] + np.arange(src_len+trg_len)])\n",
    "        sample_idx = _sample_idx[_use_sample_idx]\n",
    "        \n",
    "        ## set parameters\n",
    "        self.file_path = ecog_file\n",
    "        self.n_ch = n_ch\n",
    "        self.ch_idx = ch_idx\n",
    "        self.n_samp = n_samp\n",
    "        self.sample_idx = sample_idx\n",
    "        self.data_type = data_type\n",
    "        self.byte_per_sample = n_ch * data_type().nbytes\n",
    "        self.src_len = src_len\n",
    "        self.trg_len = trg_len\n",
    "        \n",
    "    def __len__( self ):\n",
    "        return len(self.sample_idx)\n",
    "    \n",
    "    def __getitem__( self, idx ):\n",
    "        count = (self.src_len + self.trg_len)*self.n_ch\n",
    "        offset = self.sample_idx[idx]*self.n_ch*self.data_type().nbytes\n",
    "        data_sample = np.fromfile(self.file_path,dtype=self.data_type,count=count,offset=offset).reshape((self.n_ch,self.src_len+self.trg_len),order='F').T\n",
    "        src, trg = np.split(data_sample[:,self.ch_idx],[self.src_len])\n",
    "        \n",
    "        return src, trg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it out!\n",
    "src_len = 1000\n",
    "trg_len = 500\n",
    "step_len = 500\n",
    "ecog_file = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "\n",
    "dataset = WirelessEcogDataset(ecog_file,src_len,trg_len,step_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src,trg = dataset.__getitem__(0)\n",
    "plt.plot(np.arange(dataset.src_len),src[:,0],label='src')\n",
    "plt.plot(dataset.src_len + np.arange(trg_len),trg[:,0],label='trg')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it works! now let's make a dataloader + sampler for this lil bad boy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_p = 0.8\n",
    "train_idx, valid_idx = np.split(dataset.sample_idx,[np.int(np.floor(train_p*dataset.__len__()))])\n",
    "train_loader = DataLoader(dataset,sampler=SubsetRandomSampler(train_idx),batch_size=10,shuffle=False)\n",
    "valid_loader = DataLoader(dataset,sampler=SubsetRandomSampler(valid_idx),batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECoG dataloader across multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WirelessEcogDataset_MultiFile(Dataset):\n",
    "    # Wireless ECoG Dataset\n",
    "    # Pesaran lab data\n",
    "    # AOLab\n",
    "    # Michael Nolan\n",
    "    # 2020.07.08\n",
    "    \n",
    "    def __init__( self, ecog_file_list, src_len, trg_len, step_len, ch_idx = None, device='cpu', transform=None ):\n",
    "        data_parameter_dict = self.create_parameter_dict(ecog_file_list,src_len,trg_len,step_len,ch_idx)\n",
    "        \n",
    "        ## set parameters\n",
    "        self.file_list = ecog_file_list\n",
    "#         self.n_ch = n_ch\n",
    "#         self.ch_idx = ch_idx\n",
    "#         self.n_samp = n_samp\n",
    "#         self.sample_idx = sample_idx\n",
    "#         self.data_type = data_type\n",
    "#         self.byte_per_sample = n_ch * data_type().nbytes\n",
    "        self.src_len = src_len\n",
    "        self.trg_len = trg_len\n",
    "        self.data_parameter_dict = data_parameter_dict\n",
    "        self.file_ref_idx = np.cumsum([len(x) for x in data_parameter_dict['sample_idx']])\n",
    "        self.file_offset_idx = np.zeros(self.file_ref_idx.shape,dtype=int)\n",
    "        self.file_offset_idx[1:] = self.file_ref_idx[:-1]\n",
    "        self.device = device\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__( self ):\n",
    "        return sum([len(x) for x in self.data_parameter_dict['sample_idx']])\n",
    "    \n",
    "    def __getitem__( self, idx, verbose=False ):\n",
    "        file_idx = np.arange(len(self.file_list))[idx < self.file_ref_idx][0]\n",
    "        filepath = self.file_list[file_idx]\n",
    "        in_file_sample_idx = idx - self.file_offset_idx[file_idx]\n",
    "        dtype = self.data_parameter_dict['data_type'][file_idx]\n",
    "        n_ch = self.data_parameter_dict['n_ch'][file_idx]\n",
    "        ch_idx = self.data_parameter_dict['ch_idx'][file_idx]\n",
    "        count = (self.src_len + self.trg_len)*n_ch\n",
    "        offset = self.data_parameter_dict['sample_idx'][file_idx][in_file_sample_idx]*n_ch*dtype().nbytes\n",
    "        data_sample = torch.tensor(np.fromfile(filepath,dtype=dtype,count=count,offset=offset).reshape((n_ch,self.src_len+self.trg_len),order='F').T)\n",
    "        src, trg = torch.split(data_sample[:,ch_idx],[self.src_len,self.trg_len])\n",
    "        if self.transform:\n",
    "            src, trg = self.transform(src, trg)\n",
    "        \n",
    "        return src.to(self.device,non_blocking=True), trg.to(self.device, non_blocking=True)\n",
    "    \n",
    "    def create_parameter_dict( self, ecog_file_list, src_len, trg_len, step_len, ch_idx=None ):\n",
    "        n_file = len(ecog_file_list)\n",
    "        n_samp_list = []\n",
    "        n_ch_list = []\n",
    "        srate_list = []\n",
    "        data_type_list = []\n",
    "        sample_idx_list = []\n",
    "        ch_idx_list = []\n",
    "        for file in ecog_file_list:\n",
    "            _n_samp, _n_ch, _srate, _data_type, _sample_idx, _ch_idx = self.get_ecog_file_parameters(file,src_len,trg_len,step_len,ch_idx)\n",
    "            n_samp_list.append(_n_samp)\n",
    "            n_ch_list.append(_n_ch)\n",
    "            srate_list.append(_srate)\n",
    "            data_type_list.append(_data_type)\n",
    "            sample_idx_list.append(_sample_idx)\n",
    "            ch_idx_list.append(_ch_idx)\n",
    "        parameter_dict = {\n",
    "            'n_samp': n_samp_list,\n",
    "            'n_ch': n_ch_list,\n",
    "            'srate': srate_list,\n",
    "            'data_type': data_type_list,\n",
    "            'sample_idx': sample_idx_list,\n",
    "            'ch_idx': ch_idx_list\n",
    "        }\n",
    "        return parameter_dict\n",
    "        \n",
    "    \n",
    "    def get_ecog_file_parameters( self, ecog_file, src_len, trg_len, step_len, ch_idx ):\n",
    "        ## parse file\n",
    "        data_file = os.path.basename(ecog_file)\n",
    "        data_file_kern = os.path.splitext(data_file)[0]\n",
    "        rec_id, microdrive_name, rec_type = data_file_kern.split('.')\n",
    "        data_path = os.path.dirname(ecog_file)\n",
    "        # read experiment file\n",
    "        exp_file = os.path.join(data_path,rec_id + \".experiment.json\")\n",
    "        with open(exp_file,'r') as f:\n",
    "            exp_dict = json.load(f)\n",
    "        # get microdrive parameters\n",
    "        microdrive_name_list = [md['name'] for md in exp_dict['hardware']['microdrive']]\n",
    "        microdrive_idx = [md_idx for md_idx, md in enumerate(microdrive_name_list) if microdrive_name == md][0]\n",
    "        microdrive_dict = exp_dict['hardware']['microdrive'][microdrive_idx]\n",
    "        n_ch = len(microdrive_dict['electrodes'])\n",
    "        if not ch_idx:\n",
    "            ch_idx = np.arange(n_ch)\n",
    "        # get srate\n",
    "        if rec_type == 'raw':\n",
    "            srate = experiment['hardware']['acquisition']['samplingrate']\n",
    "            data_type = np.ushort\n",
    "        elif rec_type == 'lfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        elif rec_type == 'clfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        # read mask\n",
    "        ecog_mask_file = os.path.join(data_path,data_file_kern + \".mask.pkl\")\n",
    "        with open(ecog_mask_file,\"rb\") as mask_f:\n",
    "            mask = pkl.load(mask_f)\n",
    "        mask = mask[\"hf\"] | mask[\"sat\"]\n",
    "        # get params\n",
    "        n_samp = len(mask)\n",
    "        # create sampling index - src_len+trg_len length segments that don't include a masked sample\n",
    "        _sample_idx = np.arange(n_samp-(src_len+trg_len),step=step_len)\n",
    "        _use_sample_idx = np.zeros((len(_sample_idx)),dtype=bool) # all false\n",
    "        for sidx in range(len(_sample_idx)):\n",
    "            _use_sample_idx[sidx] = ~np.any(mask[_sample_idx[sidx] + np.arange(src_len+trg_len)])\n",
    "        sample_idx = _sample_idx[_use_sample_idx]\n",
    "        \n",
    "        return n_samp, n_ch, srate, data_type, sample_idx, ch_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_file_list = glob.glob('/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/0*/*.clfp.dat')\n",
    "ecog_multifile_dataset = WirelessEcogDataset_MultiFile(ecog_file_list,src_len,trg_len,src_len+trg_len)\n",
    "print(ecog_multifile_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_src, _trg = ecog_multifile_dataset.__getitem__(1)\n",
    "src = _src - _src.mean(axis=-1)[:,None]\n",
    "trg = _trg - _trg.mean(axis=-1)[:,None]\n",
    "plot_ch_idx = 10\n",
    "spanner = 300\n",
    "fig,ax = plt.subplots(1,1,figsize=(14,9))\n",
    "ax.plot(np.arange(src_len),src + spanner*np.arange(src.shape[1]),'m');\n",
    "ax.plot(np.arange(src_len),_src.mean(axis=-1) - spanner,'r')\n",
    "ax.plot(src_len+np.arange(trg_len),trg + spanner*np.arange(trg.shape[1]),'g');\n",
    "ax.plot(src_len+np.arange(trg_len),_trg.mean(axis=-1) - spanner,'b')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('off');\n",
    "ax.set_title('Multifile Dataset - output example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "holy shit - it works! Now let's try the same train/validation loader definitions we made above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = 0.8\n",
    "train_idx, valid_idx = np.split(np.arange(ecog_multifile_dataset.__len__()),[np.int(np.floor(train_p*ecog_multifile_dataset.__len__()))])\n",
    "train_loader = DataLoader(ecog_multifile_dataset,sampler=SubsetRandomSampler(train_idx),batch_size=10,shuffle=False)\n",
    "valid_loader = DataLoader(ecog_multifile_dataset,sampler=SubsetRandomSampler(valid_idx),batch_size=10,shuffle=False)\n",
    "print('train loader size: {}'.format(len(train_loader)))\n",
    "print('validation loader size: {}'.format(len(valid_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! A multifile dataloader isn't that hard to make, turns out. This should work pretty well for the spectrogram data too, come to think of it.\n",
    "\n",
    "### Tranforms - normalizing drawn data samples\n",
    "Any given window should be normalized before passing it into the network. I can think of two different methods for implementing this:\n",
    "1. z-score the values in the given window based off of mean, variance estimates calculated from the given window\n",
    "2. z-score the values in the given window using overall file statistics\n",
    "\n",
    "The second method will require some external preprocessing, so let's take a crack at the first method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform class\n",
    "class local_zscore(object):\n",
    "    # I do not know if initialization requires more detail in this case.\n",
    "    def __init__(self):\n",
    "        None\n",
    "    \n",
    "    def __call__(self,src,trg):\n",
    "        sample = torch.cat([src,trg],dim=0)\n",
    "        mean = sample.mean(axis=0)\n",
    "        std = sample.std(axis=0)\n",
    "        src_z = (src-mean)/std\n",
    "        trg_z = (trg-mean)/std\n",
    "        return src_z, trg_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the dataset, but specify the transform\n",
    "transform = local_zscore()\n",
    "ecog_multifile_dataset = WirelessEcogDataset_MultiFile(ecog_file_list,src_len,trg_len,src_len+trg_len\n",
    "                                                      ,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_src, _trg = ecog_multifile_dataset.__getitem__(650)\n",
    "src = _src - _src.mean(axis=-1)[:,None]\n",
    "trg = _trg - _trg.mean(axis=-1)[:,None]\n",
    "plot_ch_idx = 10\n",
    "spanner = 1\n",
    "fig,ax = plt.subplots(1,1,figsize=(14,9))\n",
    "ax.plot(np.arange(src_len),src + spanner*np.arange(src.shape[1]),'m');\n",
    "ax.plot(np.arange(src_len),_src.mean(axis=-1) - spanner,'r')\n",
    "ax.plot(src_len+np.arange(trg_len),trg + spanner*np.arange(trg.shape[1]),'g');\n",
    "ax.plot(src_len+np.arange(trg_len),_trg.mean(axis=-1) - spanner,'b')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('off');\n",
    "ax.set_title('Normalized Dataset Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p = 0.8\n",
    "BATCH_SIZE = 10\n",
    "train_idx, valid_idx = np.split(np.arange(ecog_multifile_dataset.__len__()),[np.int(np.floor(train_p*ecog_multifile_dataset.__len__()))])\n",
    "train_loader = DataLoader(ecog_multifile_dataset,sampler=SubsetRandomSampler(train_idx),batch_size=BATCH_SIZE,shuffle=False)\n",
    "valid_loader = DataLoader(ecog_multifile_dataset,sampler=SubsetRandomSampler(valid_idx),batch_size=BATCH_SIZE,shuffle=False)\n",
    "print('train loader size: {}'.format(len(train_loader)))\n",
    "print('validation loader size: {}'.format(len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_batch, trg_batch = next(iter(train_loader))\n",
    "torch.cat([src_batch,trg_batch],dim=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good news, bad news: this works like a charm, but it doesn't seem to normalize the data in a way that I want. The dead channels are amplified considerably and will be injecting heaps noise into the network if not culled appropriately. I need to save ch_idx masks into the mask files then incorporate them into the classdef.\n",
    "\n",
    "Also also also the z-score isn't actually giving me mean-zero values. This should be avoidable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling:\n",
    "rand_call = lambda x: ecog_multifile_dataset.__getitem__(random.randint(0,ecog_multifile_data.__len__()-1))\n",
    "%timeit -n 1000 rand_call\n",
    "# %timeit -n 1000 next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it even runs pretty quickly! Way to go.\n",
    "\n",
    "To put the icing on the cake, let's make a function that we can pack into the package that creates the dataset from all available WirelessData files on the current compute platform. That way we can call in the dataset like we're doing the $10^{10}$th algorithm for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def spontaneous_ecog( src_len, trg_len, step_len, filter_type='clfp' ):\n",
    "    platform_name = sys.platform\n",
    "    if platform_name == 'darwin':\n",
    "        # local machine\n",
    "        data_dir_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/'\n",
    "    elif platform_name == 'linux2':\n",
    "        # HYAK, baby!\n",
    "        data_dir_path = '/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/'\n",
    "    elif platform_name == 'linux':\n",
    "        # google cloud, don't fail me now\n",
    "        data_file_full_path = '/home/mickey/Data/WirelessData/Goose_Multiscale_M1/180325/'\n",
    "    # glop the file list\n",
    "    ecog_file_list = glob.glob(os.path.join(data_dir_path,'0*/*.{}.dat').format(filter_type))\n",
    "    return WirelessEcogDataset_MultiFile(ecog_file_list,src_len,trg_len,src_len+trg_len,transform=local_zscore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_len = 1000\n",
    "trg_len = 1000\n",
    "default_ecog_dataset = spontaneous_ecog(src_len,trg_len,src_len+trg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_src, _trg = default_ecog_dataset.__getitem__(650)\n",
    "src = _src - _src.mean(axis=-1)[:,None]\n",
    "trg = _trg - _trg.mean(axis=-1)[:,None]\n",
    "plot_ch_idx = 10\n",
    "spanner = 1\n",
    "fig,ax = plt.subplots(1,1,figsize=(21,14))\n",
    "ax.plot(np.arange(src_len),src + spanner*np.arange(src.shape[1]),'m');\n",
    "ax.plot(np.arange(src_len),_src.mean(axis=-1) - spanner,'r')\n",
    "ax.plot(src_len+np.arange(trg_len),trg + spanner*np.arange(trg.shape[1]),'g');\n",
    "ax.plot(src_len+np.arange(trg_len),_trg.mean(axis=-1) - spanner,'b')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.axis('off');\n",
    "ax.set_title('Normalized Dataset Sample')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
