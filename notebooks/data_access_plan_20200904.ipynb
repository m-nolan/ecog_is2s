{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599612019232",
   "display_name": "Python 3.7.7 64-bit ('ecog_is2s': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rethinking data access: ECoG signal analysis over many, many files\n",
    "\n",
    "Michael Nolan\n",
    "\n",
    "2020.09.04.1948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend for this to become a blog post, eventually. I'll put it up on my github.io page. Should I buy a personal domain?\n",
    "\n",
    "## Outline\n",
    "- Problem\n",
    "- Data\n",
    "    - structure\n",
    "    - Filters + Masks\n",
    "- Class structure\n",
    "    - Files\n",
    "        - Sampler\n",
    "    - Dataset\n",
    "    - Dataloader\n",
    "        - multiprocessing for parallelized batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "I'm trying to train a neural network to predict time series data (more on that in a later post). I need to access the data in order to do that. I've had some success accessing individual files in the past, loading the entire file's data into memory all at once. I can easily take that file and create a `torch.util.data.Dataset` from it, but that's not too scalable when files are ~2GB a piece and I'm working with 40+ files.\n",
    "\n",
    "Previously I approached this problem by creating a dataset class that directly interfaced with the file with an unrefactored reading method. It is a little hacky, but works well for a single file. I also created a multifile class that can sample fixed-length sequences from a number of different data files through a single class interface. However, this is generally inflexible to different file types (I've created a few different pre-filtered versions of the same data) and gets the scope of file information (number of samples, channels, size of batch reads) spread across several class levels. I'd like a better interface to the data that's more flexible between different file encodings and \n",
    "\n",
    "### Solution\n",
    "Classes all the way down. This requires stable and trackable access to data distributed over many many files. I think it makes sense to make a file class that holds data file, mask and channel information in an accessible form. All data loading will be through object instances of that class. A dataset class will then be made that is constructed from a collection of data file objects. This dataset will be very flexible w.r.t. sample sizes, allowing us to make arbitrary dataloaders to ease hyperparameter sweeps.\n",
    "\n",
    "Anywho, let's go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for class defs\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os.path as path # may need to build a switch here for PC/POSIX\n",
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "# not needed for class defs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the file interface class. \n",
    "# - It will grab everything there is to know about the data file itself and it's supporting files (mask, experiment)\n",
    "#   - Channel count\n",
    "#   - Channel labels\n",
    "#   - Filtering information\n",
    "#   - Sampling rate\n",
    "#   - Sample count (recording length)\n",
    "# - It will provide a clean interface for reading data from arbitrary time points\n",
    "#   - the argumentless call will read all data from the file\n",
    "#   - arguments will change read start point and read length\n",
    "#   - arguments will change the channels returned (masked v. full)\n",
    "#   - arguments will toggle data masking (nan-pack recording)\n",
    "class DataFile():\n",
    "\n",
    "    def __init__(self, data_file_path, exp_file_path=None, mask_file_path=None):\n",
    "\n",
    "        # parse file directory and components\n",
    "        data_dir = path.dirname(data_file_path)\n",
    "        data_basename = path.basename(data_file_path)\n",
    "        rec_id, device_id, rec_type, data_ext = data_basename.split('.')\n",
    "\n",
    "        # experiment data file: construct and load\n",
    "        if not exp_file_path:\n",
    "            exp_file_name = rec_id + 'experiment.json'\n",
    "            exp_file_path = path.join(data_dir,exp_file_name)\n",
    "\n",
    "        # mask file: construct and load\n",
    "        if not mask_file_path:\n",
    "            mask_file_name = rec_id + '.' + device_id + '.' + rec_type + '.mask.pkl'\n",
    "            mask_file_path = path.join(data_dir,mask_file_name)\n",
    "         \n",
    "        # set recording parameters\n",
    "        self.set_data_parameters(data_file_path,exp_file_path,mask_file_path)\n",
    "\n",
    "    # this is returned when the print() command is called.\n",
    "    def __repr__(self):\n",
    "        path_repr_str = f'Data file object: {self.data_file_path}'\n",
    "        sample_repr_str = f'\\tsamples: {self.n_sample} ({self.n_sample/self.srate:0.2f}s, {self.data_mask.mean()*100:0.2f}% masked)'\n",
    "        ch_repr_str = f'\\tchannels: {self.n_ch} ({self.ch_idx.mean()*100:0.2f}% masked)'\n",
    "        return path_repr_str + '\\n' + sample_repr_str + '\\n' + ch_repr_str\n",
    "                \n",
    "\n",
    "    # read data segment. Default call (no arguments) returns the entire recording.\n",
    "    def read( self, t_start=0, t_len=-1, ch_idx=None, use_mask=True, mask_value=0., mask_pad_t=5 ):\n",
    "\n",
    "        # get offset sample/byte values\n",
    "        n_offset_samples = int(round(t_start * self.srate))\n",
    "        n_offset_items = n_offset_samples * self.n_ch\n",
    "        n_offset_bytes = n_offset_items * self.data_type().nbytes\n",
    "        if t_len == -1:\n",
    "            n_read_items = t_len\n",
    "            n_read_samples = self.n_sample\n",
    "        else:\n",
    "            n_read_samples = t_len * self.srate\n",
    "            n_read_items = n_read_samples * self.n_ch\n",
    "        \n",
    "        # read data\n",
    "        with open(self.data_file_path,'rb') as f:\n",
    "            data = np.fromfile(f,self.data_type,count=n_read_items,offset=n_offset_bytes).reshape(n_read_samples,self.n_ch).T\n",
    "\n",
    "        # remove channels\n",
    "        if not ch_idx:\n",
    "            ch_idx = ~self.ch_idx\n",
    "        data = data[ch_idx,:] # mask values are True for bad spots\n",
    "\n",
    "        # mask data\n",
    "        sample_idx = np.arange(n_offset_samples,n_offset_samples+n_read_samples)\n",
    "        data[:,self.data_mask[sample_idx]] = mask_value\n",
    "\n",
    "        # consider: time array? May not want to incorporate until global time is added\n",
    "        return data\n",
    "\n",
    "\n",
    "    # compute data parameter values and add as object attributes\n",
    "    def set_data_parameters( self, data_file_path, exp_file_path, mask_file_path ):\n",
    "        # parse file\n",
    "        data_file = os.path.basename(data_file_path)\n",
    "        data_file_kern = os.path.splitext(data_file)[0]\n",
    "        rec_id, microdrive_name, rec_type = data_file_kern.split('.')\n",
    "        data_path = os.path.dirname(data_file_path)\n",
    "        \n",
    "        # read experiment file\n",
    "        exp_file = os.path.join(data_path,rec_id + \".experiment.json\")\n",
    "        with open(exp_file,'r') as f:\n",
    "            exp_dict = json.load(f)\n",
    "        \n",
    "        # get microdrive parameters\n",
    "        microdrive_name_list = [md['name'] for md in exp_dict['hardware']['microdrive']]\n",
    "        microdrive_idx = [md_idx for md_idx, md in enumerate(microdrive_name_list) if microdrive_name == md][0]\n",
    "        microdrive_dict = exp_dict['hardware']['microdrive'][microdrive_idx]\n",
    "        electrode_label_list = [e['label'] for e in exp_dict['hardware']['microdrive'][0]['electrodes']]\n",
    "        n_ch = len(electrode_label_list)\n",
    "        \n",
    "        # get srate\n",
    "        if rec_type == 'raw':\n",
    "            srate = exp_dict['hardware']['acquisition']['samplingrate']\n",
    "            data_type = np.ushort\n",
    "        elif rec_type == 'lfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        elif rec_type == 'clfp':\n",
    "            srate = 1000\n",
    "            data_type = np.float32\n",
    "        \n",
    "        # read mask\n",
    "        ecog_mask_file = os.path.join(data_path,data_file_kern + \".mask.pkl\")\n",
    "        with open(ecog_mask_file,\"rb\") as mask_f:\n",
    "            mask = pkl.load(mask_f)\n",
    "        data_mask = mask[\"hf\"] | mask[\"sat\"]\n",
    "        if 'ch' in mask.keys():\n",
    "            ch_idx = mask['ch']\n",
    "        else:\n",
    "            ch_idx = np.arange(n_ch)\n",
    "        \n",
    "        # set parameters\n",
    "        self.data_file_path = data_file_path\n",
    "        self.exp_file_path = exp_file_path\n",
    "        self.mask_file_path = mask_file_path\n",
    "        self.rec_id = rec_id\n",
    "        self.microdrive_name = microdrive_name\n",
    "        self.rec_type = rec_type\n",
    "        self.srate = srate\n",
    "        self.data_type = data_type\n",
    "        self.data_mask = data_mask\n",
    "        self.n_ch = n_ch\n",
    "        self.ch_idx = ch_idx\n",
    "        self.ch_labels = electrode_label_list\n",
    "\n",
    "        # set sample length information\n",
    "        self.n_sample = len(self.data_mask)\n",
    "        self.t_total = self.n_sample/self.srate # (s)\n",
    "\n",
    "    # morphological transform: expand boolean array values a set distance from all current True values.\n",
    "    # this is used to spread out mask estimates to more conservatively filter data files.\n",
    "    def grow_bool_array( in_bool_array, growth_size=50.):\n",
    "        None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should do the trick! Let's build a tester to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test class instance for the first recording in the dataset:\n",
    "data_file_path = \"E:\\\\aoLab\\\\Data\\\\WirelessData\\\\Goose_Multiscale_M1\\\\180325\\\\001\\\\rec001.LM1_ECOG_3.clfp.dat\"\n",
    "datafile = DataFile(data_file_path)\n",
    "print(datafile)\n",
    "t_start = 7000\n",
    "t_len = 1\n",
    "data_sample = datafile.read(t_start=t_start,t_len=t_len)\n",
    "plt.plot(data_sample[11,:]);\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude ($\\mu$V)')\n",
    "plt.title('Sample data segment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! It works great. I even added a nice `self.__repr__()` method to format its output when passed to python's `print` function. \n",
    "\n",
    "Now let's get a sense of the read speeds. I'll repeat the `datafile.read()` call with random time points 1000 times and get some statistics on average loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "n_iter = 1000\n",
    "t_len = 1\n",
    "t_iter = np.zeros(n_iter)\n",
    "for k in range(n_iter):\n",
    "    t_start = np.random.rand()*(datafile.t_total-t_len)\n",
    "    t = time.time()\n",
    "    data = datafile.read(t_start=t_start,t_len=t_len)\n",
    "    t_iter[k] = time.time()-t\n",
    "print(f'{n_iter} data pulls complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(t_iter,100,label='read times');\n",
    "plt.axvline(t_iter.mean(),color='r',label='mean')\n",
    "plt.xscale('linear')\n",
    "plt.xlabel('Read time (s)')\n",
    "plt.ylabel('sample frequency')\n",
    "plt.title('1s sample read time distribution - DataFile class')\n",
    "plt.legend(loc=0)\n",
    "print(f'Mean read time: {t_iter.mean():0.3E}')\n",
    "print(f'Total read time ({n_iter} reads): {t_iter.sum():0.3E}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! I'm unsure of what's causing the split distribution in read times here, but we're getting a very, very fast read speed for >80% of draws. \n",
    "\n",
    "The total batch read time is 0.16s. If we're to use this as the data access API for a pytorch dataloader, we might want something faster. This file is currently on an SSD connected to my computer through a USBC/3.0 port. Moving all of this data to my system's internal memory may provide me with an immediate speed-up, [provided that I use my PCIe SSD.](https://www.unbxtech.com/2019/03/pcie-sata-usb-interfaces-explained.html). Using different data reading methods to optimize the code [may not lead to much significant improvement, so I'll leave them be for now.](http://rabexc.org/posts/io-performance-in-python).\n",
    "\n",
    "Either way, it all works great! This will make the next step a bit easier: creating a dataset interface to a list of these DataFile objects. Coming soon!"
   ]
  }
 ]
}