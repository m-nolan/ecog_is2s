{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import progressbar as pb\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5050\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now let's build a loader for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aopy import datareader, datafilter\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']\n",
    "# we already found the appropriate data masks, so just load them in\n",
    "mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "with open(mask_file_path, 'rb') as f:\n",
    "    mask_data = pkl.load(f)\n",
    "hf_mask = mask_data[\"hf\"]\n",
    "sat_mask = mask_data[\"sat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot data segment for a nice tidy figure\n",
    "# import matplotlib.pyplot as pp\n",
    "# t_plot = np.arange(data_in.shape[1])/srate_in\n",
    "# t_plot.shape\n",
    "\n",
    "# t_start = 70\n",
    "# t_end = 80\n",
    "# plot_idx = range(srate_in*t_start,srate_in*t_end)\n",
    "# n_chan_plot = 10\n",
    "# ch_idx = range(n_chan_plot)\n",
    "\n",
    "# fig, ax = pp.subplots(figsize=(3,8))\n",
    "# ax.plot(data_in[0:n_chan_plot*6:6,plot_idx].transpose() + 1500*np.arange(n_chan_plot),t_plot[plot_idx])\n",
    "# ax.set_ylim((t_start,t_end))\n",
    "# fig.patch.set_visible(False)\n",
    "# ax.set_xlabel('ECoG Data')\n",
    "# ax.axis('off')\n",
    "\n",
    "# with open(\"ECoG_trace.png\", 'wb') as outfile:\n",
    "#     fig.canvas.print_png(outfile)\n",
    "data_in.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ind = ~np.logical_or(hf_mask,sat_mask)\n",
    "plt.plot(np.arange(data_in.shape[1])[plot_ind], data_in[-1,plot_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask data array, remove obvious outliers\n",
    "data_in[:,np.logical_or(hf_mask,sat_mask)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re(down)sample data\n",
    "# srate = 250 # not much signal above 100Hz\n",
    "# ds_factor = np.intc(np.floor(srate_in/srate)) # decimate does not allow for floats as ds_fac arguments\n",
    "# data_in = sp.signal.decimate(data_in,ds_factor,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "with open(mask_file_path, 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EcogDataset(Dataset):\n",
    "    def __init__(self, data_in, block_len):\n",
    "        self.data = data_in\n",
    "        self.block_len = int(block_len)\n",
    "        self.data_len = self.data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] // self.block_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get data range (make sure not to sample from outside range)\n",
    "        return self.data[idx:(idx + self.block_len),:]\n",
    "        # an attempt at zero-padding to fix the tensor cat issue.\n",
    "#         smpl = self.data[idx:(idx + self.block_len),:]\n",
    "#         if smpl.shape[0] < self.block_len:\n",
    "#             smpl_ = smpl\n",
    "#             smpl = np.zeros((self.block_len,smpl_.shape[1]),dtype=np.float)\n",
    "#             smpl[:smpl_.shape[0],:] = smpl_\n",
    "#         return smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "srate = srate_in\n",
    "# data_in = np.double(data_in[:,:120*srate])\n",
    "enc_len = 10\n",
    "dec_len = 1\n",
    "seq_len = enc_len+dec_len # use ten time points to predict the next time point\n",
    "\n",
    "unit_convert_uV_V = 1.e-6\n",
    "# dataset = pd.DataFrame(data_in.transpose(),dtype=np.double) # may be unnecessary for now, but df will probably help combine files in the future.\n",
    "# datareader.load_ecog_clfp_data.get_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.from_numpy(sp.stats.zscore(data_in[:,data_size[1]//2:].view().transpose()))\n",
    "print(data_tensor.size)\n",
    "dataset = EcogDataset(data_tensor,seq_len) ## make my own Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "block_time = 20\n",
    "block_size = block_time*srate\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "data_size = np.shape(data_tensor)[0]\n",
    "n_block = np.floor(data_size/block_size)\n",
    "idx_all = np.arange(data_size)\n",
    "smpl_idx_all = idx_all[:-seq_len:seq_len]\n",
    "n_samp = len(smpl_idx_all)\n",
    "shuffle_idx = torch.randperm(n_samp)\n",
    "train_split = int(np.floor(train_frac*n_samp))\n",
    "valid_split = int(np.floor(valid_frac*n_samp))\n",
    "test_split = int(np.floor(test_frac*n_samp))\n",
    "# rework this using\n",
    "train_idx = smpl_idx_all[shuffle_idx[:train_split]]\n",
    "valid_idx = smpl_idx_all[shuffle_idx[train_split:train_split+valid_split]]\n",
    "test_idx = smpl_idx_all[shuffle_idx[train_split+valid_split:-1]]\n",
    "print(train_idx.shape,valid_idx.shape,test_idx.shape)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=train_sampler,\n",
    "                                           drop_last=True) # this can be avoided using some padding sequence classes, I think\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                          sampler=test_sampler,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your DataLoader is working correctly, this should be `torch.Size([<seq_len>, <num_ch>])\n",
    "dataset.__getitem__(next(iter(train_sampler))).shape\n",
    "# data_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### back to our data (potenially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, valid_data, test_data),\n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # gated recurrent layer, dropout layer\n",
    "        self.rnn = nn.GRU(input_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        # note: batch_first only permutes dimension order in input and output tensors. It does not affect hidden state.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # input_data: [batch_size x seq_len x input_dim]\n",
    "        # h0: [n_layers x batch_size x hid_dim]\n",
    "        batch_size = input_data.size(0)\n",
    "#         hidden = torch.randn(self.n_layers, batch_size, self.hid_dim) # initialize hidden layer value\n",
    "        output, hidden = self.rnn(input_data) # hidden initialized as zero tensor\n",
    "            \n",
    "        # output = [batch_size x seq_len x hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        # input = [batch_size, seq_len, hid_dim]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "#         input_data = input_data.unsqueeze(0) # not sure if this this is needed for not-embedded inputs\n",
    "        if len(input_data.size()) != 3 or len(hidden.size()) != 3:\n",
    "            breakpoint()\n",
    "        output, hidden = self.rnn(input_data, hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #\"seq len and n directions will always be 1 in the decoder, therefore:\" <- figure out how to change this\n",
    "        #output = [batch_size, 1, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction, output, hidden # predicted ECoG signal, decoder states, last decoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device # what is this?\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Encoder, decoder embedding dimensions (hidden state) must be equal.\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder, decoder layer number must be equal.\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio: prob. to use teacher forcing\n",
    "        #e.g. if 0.75, ground-truth imports are used 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        \n",
    "        src_len = src.shape[1]\n",
    "        src_dim = src.shape[2]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_dim = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_dim).to(self.device)\n",
    "        \n",
    "        enc_state, hidden = self.encoder(src)\n",
    "        \n",
    "        output = src[:,-1,:].unsqueeze(1) # start the decoder with the actual output\n",
    "        \n",
    "        for t in range(trg_len): # ignore that first data point\n",
    "            pred, output, hidden = self.decoder(output,hidden)\n",
    "            \n",
    "            outputs[:,t,:] = pred.squeeze()\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            input = trg[:,t,:] if teacher_force else output\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SEQ_LEN = 10 \n",
    "OUTPUT_SEQ_LEN = 1 # predict one output state from 10 inputs prior\n",
    "INPUT_DIM = num_ch\n",
    "OUTPUT_DIM = num_ch\n",
    "HID_DIM = num_ch\n",
    "N_ENC_LAYERS = 1 \n",
    "N_DEC_LAYERS = 1\n",
    "ENC_DROPOUT = np.float32(0.5)\n",
    "DEC_DROPOUT = np.float32(0.5)\n",
    "\n",
    "enc = Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq_GRU(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of enc/dec function\n",
    "# # let's pass the first pop off the dataset to the encoder and look at the outputs\n",
    "# enc_out, hid_enc = enc.forward(test_loader.__iter__()._next_data())\n",
    "# # out: [h1, h2, ..., h{seq_len}]\n",
    "# # hid: h{seq_len}\n",
    "# print(enc_out.size(),hid_enc.size())\n",
    "# est, dec_out, hid_dec = dec.forward(enc_out,hid_enc)\n",
    "# print(est,dec_out.size(),hid_dec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forward pass\n",
    "# get first data pull\n",
    "# dataset.__getitem__(next(iter(train_sampler)))\n",
    "data_batch = next(iter(train_loader))\n",
    "src = data_batch[:,:enc_len,:]\n",
    "trg = data_batch[:,enc_len:,:]\n",
    "print(src.size(),trg.size())\n",
    "test_out = model(src,trg)\n",
    "print(test_out.size()) # it actually works!\n",
    "criterion(test_out,trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    batch_loss = []\n",
    "#     widgets = [pb.Percentage(), progressbar.Bar()]\n",
    "#     bar = pb.ProgressBar(widgets=widgets).start()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        if np.mod(i+1,1000) == 0:\n",
    "            print(i,len(iterator))\n",
    "        src = batch[:,:-1,:]\n",
    "        trg = batch[:,-1,:].unsqueeze(1) # otherwise it would automatically cut this out.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [batch size, trg len, output dim]\n",
    "        #output = [batch size, trg len, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "#         output = output[1:].view(-1, output_dim)\n",
    "#         trg = trg[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "#         bar.update(10*i/10000)\n",
    "\n",
    "#         if i > 10000:\n",
    "#             break\n",
    "        \n",
    "    return epoch_loss / len(iterator), np.array(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    batch_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         widgets = [pb.Percentage(), progressbar.Bar()]\n",
    "#         bar = pb.ProgressBar(widgets=widgets).start()\n",
    "#         i = 0\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            if np.mod(i+1,1000)==0:\n",
    "                print(i,len(iterator))\n",
    "            src = batch[:,:-1,:]\n",
    "            trg = batch[:,-1,:].unsqueeze(1)\n",
    "\n",
    "            output = model(src, trg, 0.) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "#             bar.update(i/10000)\n",
    "\n",
    "#             if i > 10000:\n",
    "#                 break\n",
    "#             i += 1\n",
    "        \n",
    "    return epoch_loss / len(iterator), np.array(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.encoder.rnn.dropout))\n",
    "# result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
    "# --> 716          self.dropout, self.training, self.bidirectional, self.batch_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "train_loss = np.zeros(N_EPOCHS)\n",
    "train_batch_loss = []\n",
    "test_loss = np.zeros(N_EPOCHS)\n",
    "test_batch_loss = []\n",
    "\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.add_subplot(1,1,1)\n",
    "\n",
    "for e_idx, epoch in enumerate(range(N_EPOCHS)):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Training Network:')\n",
    "    train_loss[e_idx], trbl_ = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    train_batch_loss.append(trbl_)\n",
    "    print('Testing Network:')\n",
    "    test_loss[e_idx], tebl_ = evaluate(model, test_loader, criterion)\n",
    "    test_batch_loss.append(tebl_)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss[e_idx] < best_test_loss:\n",
    "        best_test_loss = test_loss[e_idx]\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss[e_idx]:.3g}')\n",
    "    print(f'\\t Val. Loss: {test_loss[e_idx]:.3g}')\n",
    "    \n",
    "    ax.plot(e_idx,train_loss[e_idx],'b.')\n",
    "    ax.plot(e_idx,test_loss[e_idx],'r.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "valid_loss = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss[-1]:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model\n",
    "\n",
    "# pack this as a method into the seq2seq class later!\n",
    "\n",
    "model_save_dir = \"/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq\"\n",
    "datetime_str = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "model_save_file_name = 'test_network_model_' + datetime_str + '.pt'\n",
    "model_save_path = os.path.join(model_save_dir,model_save_file_name)\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'valid_loss': valid_loss\n",
    "            },model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
