{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('ecog_is2s': conda)",
   "display_name": "Python 3.7.7 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Day 1 ECoG Seq2seq\n",
    "\n",
    "Michael Nolan\n",
    "\n",
    "2020.09.16"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from os import makedirs, chmod\n",
    "import glob\n",
    "import functools\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "import aopy\n",
    "import ecog_is2s\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Getting Data squared away\n",
    "Define dataset from list of files, then get train/valid/test loaders"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file list to dataset\n",
    "data_path_root = 'C:\\\\Users\\\\mickey\\\\aoLab\\\\Data\\\\WirelessData\\\\Goose_Multiscale_M1'\n",
    "data_path_day = path.join(data_path_root,'180326')\n",
    "data_file_list = glob.glob(path.join(data_path_day,'*\\\\*ECOG*clfp_ds250_fl0u10.dat'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('mounting to device: {}'.format(device))\n",
    "print(f'files found:\\t{len(data_file_list)}')\n",
    "print(f'files: {data_file_list}')\n",
    "datafile_list = [aopy.data.DataFile(df) for df in data_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_t = 0.5\n",
    "trg_t = 0.2\n",
    "step_t = src_t+trg_t\n",
    "diff_transform = ecog_is2s.Util.add_signal_diff() # no need for the srate parameter, dx est. is z-scored as well\n",
    "zscore_transform = ecog_is2s.Util.local_zscore()\n",
    "transform = lambda sample : diff_transform(zscore_transform(sample))\n",
    "dfds_list = [aopy.data.DatafileDataset(df,src_t,trg_t,step_t,device=device) for df in datafile_list]\n",
    "datafile_concatdataset = aopy.data.DatafileConcatDataset(dfds_list,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = (4,1,1)\n",
    "batch_size = 500\n",
    "train_loader, valid_loader, test_loader = datafile_concatdataset.get_data_loaders(partition=partition,batch_size=batch_size,rand_part=True)"
   ]
  },
  {
   "source": [
    "## Create model\n",
    "Construct seq2seq network and training apparatus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_ch = datafile_concatdataset.n_ch\n",
    "n_unit = 2**10\n",
    "n_layers = 1\n",
    "dropout = 0.0\n",
    "use_diff = True\n",
    "bidirectional = False\n",
    "model = ecog_is2s.Seq2Seq.Seq2Seq_GRU(input_dim=n_ch,hid_dim=n_unit,n_layers=n_layers,enc_len=0,dec_len=0,device=device,dropout=dropout,use_diff=use_diff,bidirectional=bidirectional).to(device)\n",
    "print(f'The model has {ecog_is2s.Util.count_parameters(model):,} trainable parameters')\n",
    "LOSS_OBJ = 'MSE'\n",
    "LEARN_RATE = 1e-4\n",
    "LR_SCHEDULE_FACTOR = 0.2\n",
    "TFR = 0.0\n",
    "CLIP = 2.0\n",
    "criterion = ecog_is2s.Training.ECOGLoss(objective=LOSS_OBJ)\n",
    "# criterion = torch.nn.modules.loss.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARN_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=LR_SCHEDULE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prep constants\n",
    "N_EPOCHS = 250\n",
    "epoch_train_loss = np.zeros(N_EPOCHS)\n",
    "epoch_valid_loss = np.zeros(N_EPOCHS)\n",
    "best_train_loss = np.inf\n",
    "best_valid_loss = np.inf\n",
    "epoch_time = np.zeros(N_EPOCHS)\n",
    "# create session directory\n",
    "model_save_dir = \"D:\\\\Users\\\\mickey\\\\Data\\\\models\\\\pyt\\\\seq2seq\"\n",
    "# model_dir = \"enc1.0_dec0.5_srate250_20200924221626\"\n",
    "model_dir = f\"enc{src_t}_dec{trg_t}_srate{datafile_concatdataset.srate}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "checkpoint_save_path = path.join(model_save_dir,model_dir)\n",
    "if path.exists(checkpoint_save_path): # continue training an existing model\n",
    "    # load previous checkpoint, if you can, and initialize the model\n",
    "    checkpoint_state_dict = torch.load(path.join(checkpoint_save_path,'checkpoint.pt'))\n",
    "    if 'model_state_dict' in checkpoint_state_dict: # modern save format\n",
    "        model.load_state_dict(checkpoint_state_dict['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint_state_dict['optimizer_state_dict'])\n",
    "        criterion.load_state_dict(checkpoint_state_dict['criterion_state_dict'])\n",
    "        epoch_start = checkpoint_state_dict['epoch']\n",
    "    else: # old save format, model state dict only\n",
    "        model.load_state_dict(checkpoint_state_dict)\n",
    "        epoch_start = 0\n",
    "    model.to(device) # device load may not be necessary, JIC\n",
    "    # future implementations: save the optimizer, scheduler and criterion states as well.\n",
    "else: # train a new model\n",
    "    makedirs(checkpoint_save_path,mode=0o777)\n",
    "    epoch_start = 0\n",
    "for epoch_idx, epoch in tqdm.tqdm(enumerate(range(epoch_start,epoch_start+N_EPOCHS))):\n",
    "    _t = time.time()\n",
    "    # get data loaders\n",
    "    train_loader, valid_loader, test_loader = datafile_concatdataset.get_data_loaders(partition=partition,batch_size=batch_size,rand_part=True)\n",
    "    # forward pass\n",
    "    _, trbl_ = model.train_iter(train_loader,optimizer,criterion,clip=CLIP,teacher_forcing_ratio=TFR)\n",
    "    epoch_train_loss[epoch_idx] = trbl_.mean()\n",
    "    _, vabl_ = model.eval_iter(valid_loader,criterion)\n",
    "    epoch_valid_loss[epoch_idx] = vabl_.mean()\n",
    "    epoch_time[epoch_idx] = time.time() - _t\n",
    "    # if validation loss has decreased, save the checkpoint\n",
    "    if epoch_valid_loss[epoch_idx] < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss[epoch_idx]\n",
    "        checkpoint_file_path = path.join(checkpoint_save_path,'checkpoint.pt')\n",
    "        checkpoint_state_dict = {\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'scheduler_state_dict' : scheduler.state_dict(),\n",
    "            'criterion_state_dict' : criterion.state_dict(),\n",
    "            'epoch' : epoch,\n",
    "            'valid_loss' : best_valid_loss,\n",
    "            'input_dim' : model.input_dim,\n",
    "            'hid_dim' : model.hid_dim,\n",
    "            'bidir' : model.bidirectional,\n",
    "            'use_diff' : model.use_diff\n",
    "        }\n",
    "        with open(checkpoint_file_path,'wb') as f:\n",
    "            torch.save(checkpoint_state_dict,f)\n",
    "    # update the scheduler\n",
    "    scheduler.step(epoch_valid_loss[epoch_idx],epoch_idx)\n",
    "    # update the loss figure\n",
    "    loss_fig,loss_ax = plt.subplots(1,1,dpi=100)\n",
    "    loss_ax.plot(epoch_train_loss[:epoch_idx+1],'b.',label='train')\n",
    "    loss_ax.plot(epoch_valid_loss[:epoch_idx+1],'r.',label='validation')\n",
    "    loss_ax.set_xlabel('epochs')\n",
    "    loss_ax.set_ylabel('loss (MSE)')\n",
    "    loss_ax.set_title('Learning Plots, Seq2seq model')\n",
    "    loss_ax.legend(loc=0)\n",
    "    loss_fig.savefig(path.join(checkpoint_save_path,'training_loss.png'))\n",
    "    plt.close(loss_fig)\n"
   ]
  }
 ]
}