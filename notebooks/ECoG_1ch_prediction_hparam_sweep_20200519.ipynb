{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECoG Forecasting with Sequence-to-Sequence (seq2seq) RNN models.\n",
    "\n",
    "## Starting small: 1 channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from aopy import datareader, datafilter\n",
    "from ecog_is2s import EcogDataloader, Training, Encoder, Decoder, Seq2Seq, Util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SequentialSampler, BatchSampler, SubsetRandomSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "import scipy as sp\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# import progressbar as pb\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from itertools import product\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell is used to parse command line arguments. These arguments assign values to the network and training parameters. This functionality has been replaced with more hard-coded constants in the current notebook. From an organizational standpoint, that's not the worst thing - it's given me a good opportunity to collect all of the constants together into one single code block @ the top of the notebook. They were pretty scattershot before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grab input arguments\n",
    "# parser = argparse.ArgumentParser('Trains a seq2seq network on a section of example NHP PMC ECoG data.',add_help=True)\n",
    "# parser.add_argument('--encoder-depth', metavar='el', type=int, default=10, help='Sequence depth of the encoder network')\n",
    "# parser.add_argument('--decoder-depth', metavar='dl', type=int, default=1, help='Sequence depth of the decoder network')\n",
    "# parser.add_argument('--batch-size', metavar='b', type=int, default=1, help='Data batch size')\n",
    "# parser.add_argument('--num-epochs', metavar='n', type=int, default=1, help='Number of optimization epochs')\n",
    "# parser.add_argument('--num-layers', metavar='nl', type=int, default=1, help='Number of layers in each RNN block')\n",
    "\n",
    "# args = parser.parse_args() # this bad boy has all the values packed into it. Nice!\n",
    "# print(args.encoder_depth,args.decoder_depth)\n",
    "\n",
    "# print(args.encoder_depth,args.decoder_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO:\n",
    "Clean this mess up! There are lots of copied values and variable rereferences. They're unncessary and confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "T_MINUTES = 2\n",
    "ENCODER_DEPTH = 250\n",
    "DECODER_DEPTH = 50\n",
    "n_units = [1028, 512, 256, 128]\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 500\n",
    "N_EPOCHS = NUM_EPOCHS\n",
    "n_layers = [1, 2, 3,]\n",
    "RNG_SEED = 5050\n",
    "INPUT_SEQ_LEN = ENCODER_DEPTH\n",
    "OUTPUT_SEQ_LEN = DECODER_DEPTH\n",
    "N_CH_USE = 1\n",
    "dropout = [0.0, 0.1, 0.2, 0.3,]\n",
    "l_rate = [0.0001, 0.0005, 0.001]\n",
    "LOSS_OBJ = 'MSE' #L1, L2, see training.py:ECOGLoss()\n",
    "WEIGHT_RANGE = (-0.2,0.2) # ignore for now; not sure how to worm this through\n",
    "train_frac = 0.8\n",
    "test_frac = 1 - train_frac\n",
    "valid_frac = 0.0\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# N_EPOCHS = args.num_epochs\n",
    "CLIP = 1. # this the maximum norm of the whole parameter gradient.\n",
    "TFR = 0. # no teacher forcing! Anything it's learning is all on its own\n",
    "RAND_SAMP = False\n",
    "weight_reg = 0.#0.0003\n",
    "enc_len = ENCODER_DEPTH\n",
    "dec_len = DECODER_DEPTH\n",
    "seq_len = ENCODER_DEPTH+DECODER_DEPTH # use ten time points to predict the next time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed RNG for pytorch/np\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "torch.cuda.manual_seed(RNG_SEED)\n",
    "torch.backends.cudnn.deterministic = True # enforces deterministic algorithm use -> reproducibility. Remove for production code. You don't do production code. Don't remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mounting to device: cpu\n"
     ]
    }
   ],
   "source": [
    "# set device - CUDA if you've got it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('mounting to device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file:\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "platform_name = sys.platform\n",
    "if platform_name == 'darwin':\n",
    "    # local machine\n",
    "    data_file_full_path = '/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = \"/Volumes/Samsung_T5/aoLab/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "    model_save_dir_path = '/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq/'\n",
    "elif platform_name == 'linux2':\n",
    "    # HYAK, baby!\n",
    "    data_file_full_path = '/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = \"/gscratch/stf/manolan/Data/WirelessData/Goose_Multiscale_M1/180325/001/rec001.LM1_ECOG_3.clfp.mask.pkl\"\n",
    "elif platform_name == 'linux':\n",
    "    # google cloud, don't fail me now\n",
    "    data_file_full_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.dat'\n",
    "    mask_file_path = '/home/mickey/rec001.LM1_ECOG_3.clfp.mask.pkl'\n",
    "    model_save_dir_path = '/home/mickey/models/pyt/seq2seq/'\n",
    "\n",
    "# make sure the output directory actually exists\n",
    "if not os.path.exists(model_save_dir_path):\n",
    "    os.makedirs(model_save_dir_path)\n",
    "\n",
    "data_in, data_param, data_mask = datareader.load_ecog_clfp_data(data_file_name=data_file_full_path)\n",
    "srate_in= data_param['srate']\n",
    "num_ch = data_param['num_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling data from 1000 to 250\n",
      "Data Size:\t(62, 30000)\n",
      "\n",
      "Filtering Channels:\n",
      "Num. ch. used:\t56\n",
      "Ch. kept:\t[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57]\n",
      "\n",
      "Normalizing data, converting to tensor:\n",
      "Data tensor shape: torch.Size([30000, 56])\n",
      "\n",
      "Creating EcogDataloader dataset object:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mask data array, remove obvious outliers\n",
    "data_in[:,np.logical_or(data_mask[\"hf\"],data_mask[\"sat\"])] = 0.\n",
    "\n",
    "# downsample data\n",
    "srate_down = 250\n",
    "srate = srate_in\n",
    "\n",
    "# grab local time segment\n",
    "total_len_T = T_MINUTES*60\n",
    "total_len_n = total_len_T*srate_in\n",
    "data_idx = data_in.shape[1]//2 + np.arange(total_len_n)\n",
    "print('Downsampling data from {0} to {1}'.format(srate_in,srate_down))\n",
    "### note: this breaks mask indexing, if you plan to do that later.\n",
    "data_in = np.float32(sp.signal.decimate(data_in[:,data_idx],srate_in//srate_down,axis=-1))\n",
    "print('Data Size:\\t{}\\n'.format(data_in.shape))\n",
    "\n",
    "# filter dead channels\n",
    "ch_rms = np.std(data_in,axis=-1)\n",
    "ch_m = np.mean(ch_rms)\n",
    "ch_low_lim = ch_m - 2*np.std(ch_rms)\n",
    "ch_up_lim = ch_m + 2*np.std(ch_rms)\n",
    "ch_idx = np.logical_and(ch_rms > ch_low_lim, ch_rms < ch_up_lim)\n",
    "ch_list = np.arange(num_ch)[ch_idx]\n",
    "num_ch_down = len(ch_list)\n",
    "print('Filtering Channels:')\n",
    "print('Num. ch. used:\\t{}'.format(num_ch_down))\n",
    "print('Ch. kept:\\t{}\\n'.format(ch_list))\n",
    "\n",
    "#create data tensor\n",
    "print('Normalizing data, converting to tensor:')\n",
    "data_rail = np.max(np.abs(data_in.reshape(-1)))\n",
    "# normalization = 'zscore'\n",
    "normalization = 'tanh'\n",
    "if normalization is 'max':\n",
    "    data_tensor = torch.from_numpy(data_in[ch_idx,:].view().transpose()/data_rail)\n",
    "elif normalization is 'zscore':\n",
    "    # for nominally gaussian data distributions, this will get ~99% of data points in (-1, 1)\n",
    "    data_tensor = torch.from_numpy(sp.stats.zscore(data_in[ch_idx,:].view().transpose())/5)\n",
    "elif normalization is 'tanh':\n",
    "    data_tensor = torch.from_numpy(np.tanh(sp.stats.zscore(data_in[ch_idx,:].view().transpose())/3))\n",
    "print('Data tensor shape: {}\\n'.format(data_tensor.shape))\n",
    "\n",
    "# create dataset object\n",
    "print('Creating EcogDataloader dataset object:')\n",
    "if device == 'cuda:0':\n",
    "    data_tensor.cuda()\n",
    "dataset = EcogDataloader.EcogDataset(data_tensor[:,:N_CH_USE],device,seq_len) ## make my own Dataset class\n",
    "num_ch_down = dataset.n_ch\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sampling index sets\n",
    "idx_all = np.arange(dataset.data.shape[0])\n",
    "idx_step = int(np.round(0.1*srate_down))\n",
    "sample_idx = idx_all[:-seq_len:idx_step]\n",
    "# plot samples\n",
    "n_plot_seed = 1\n",
    "n_plot_step = 4*seq_len\n",
    "plot_seed_idx = np.arange(0,n_plot_seed*n_plot_step,n_plot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the network!\n",
    "# INPUT_DIM = num_ch_down\n",
    "# OUTPUT_DIM = num_ch_down\n",
    "# HID_DIM = NUM_HID_DIM\n",
    "\n",
    "# enc = Encoder.Encoder_GRU(INPUT_DIM, HID_DIM, N_ENC_LAYERS, INPUT_SEQ_LEN, ENC_DROPOUT)\n",
    "# dec = Decoder.Decoder_GRU(OUTPUT_DIM, HID_DIM, N_DEC_LAYERS, OUTPUT_SEQ_LEN, DEC_DROPOUT)\n",
    "\n",
    "# model = Seq2Seq.Seq2Seq_GRU(enc, dec, device).to(device)\n",
    "# model.apply(Util.init_weights) # initialize the model each time.\n",
    "\n",
    "# print(f'The model has {Util.count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# criterion = Training.ECOGLoss(sum_axis=1,objective=LOSS_OBJ)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=LEARN_RATE,weight_decay=weight_reg)\n",
    "# def create_seq2seq_model(input_dim,hidden_dim,output_dim,in_seq_len,out_seq_len,dropout=ENC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple progressbar, not tied to the iterator\n",
    "def print_progress_bar(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training + evaluation function, run this in each parameter choice loop\n",
    "def train_eval_seq2seq(dataset,n_layers,n_units,l_rate,dropout,save_dir,dry_run=False):\n",
    "    \n",
    "    ENCODER_DEPTH = 250\n",
    "    DECODER_DEPTH = 50\n",
    "    BATCH_SIZE = 100\n",
    "    NUM_EPOCHS = 500\n",
    "    N_EPOCHS = NUM_EPOCHS\n",
    "    INPUT_SEQ_LEN = ENCODER_DEPTH\n",
    "    OUTPUT_SEQ_LEN = DECODER_DEPTH\n",
    "    INPUT_DIM = dataset.data.shape[-1]\n",
    "    OUTPUT_DIM = dataset.data.shape[-1]\n",
    "    LOSS_OBJ = 'MSE' #L1, L2, see training.py:ECOGLoss()\n",
    "    WEIGHT_RANGE = (-0.2,0.2) # ignore for now; not sure how to worm this through\n",
    "    train_frac = 0.8\n",
    "    test_frac = 1 - train_frac\n",
    "    valid_frac = 0.0\n",
    "    CLIP = 1.\n",
    "    TFR = 0.\n",
    "    RAND_SAMP = False\n",
    "    weight_reg = 0\n",
    "    enc_len = ENCODER_DEPTH\n",
    "    dec_len = DECODER_DEPTH\n",
    "    seq_len = ENCODER_DEPTH+DECODER_DEPTH # use ten time points to predict the next time point\n",
    "    \n",
    "    # create model from given spec\n",
    "    enc = Encoder.Encoder_GRU(INPUT_DIM, n_units, n_layers, INPUT_SEQ_LEN, dropout)\n",
    "    dec = Decoder.Decoder_GRU(OUTPUT_DIM, n_units, n_layers, OUTPUT_SEQ_LEN, dropout)\n",
    "\n",
    "    model = Seq2Seq.Seq2Seq_GRU(enc, dec, device).to(device)\n",
    "    model.apply(Util.init_weights) # initialize the model each time.\n",
    "\n",
    "    print(f'The model has {Util.count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    criterion = Training.ECOGLoss(objective=LOSS_OBJ)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=l_rate,weight_decay=weight_reg)\n",
    "    \n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    train_loss = np.zeros(N_EPOCHS)\n",
    "    train_batch_loss = []\n",
    "    test_loss = np.zeros(N_EPOCHS)\n",
    "    test_batch_loss = []\n",
    "\n",
    "    # create training session directory\n",
    "    time_str = Util.time_str()\n",
    "#     session_save_path = os.path.join(model_save_dir_path,'enc{}_dec{}_nl{}_nep{}_{}'.format(enc_len,dec_len,N_ENC_LAYERS,N_EPOCHS,time_str))\n",
    "    param_session_name = 'model_{}'.format(time_str)\n",
    "    session_save_path = os.path.join(save_dir,param_session_name)\n",
    "    sequence_plot_path = os.path.join(session_save_path,'example_sequence_figs')\n",
    "    os.makedirs(session_save_path) # no need to check; there's no way it exists yet.\n",
    "    os.makedirs(sequence_plot_path)\n",
    "    print('saving session data to:\\t{}'.format(session_save_path))\n",
    "\n",
    "    if dry_run:\n",
    "        train_loss = [np.nan]\n",
    "        test_loss = [np.nan]\n",
    "    else:\n",
    "        # make figure (and a place to save it)\n",
    "        f_loss = plt.figure()\n",
    "        ax_loss = f_loss.add_subplot(1,1,1)\n",
    "\n",
    "        for e_idx, epoch in enumerate(range(N_EPOCHS)):\n",
    "            print_progress_bar(epoch,N_EPOCHS,status='epoch:\\t{}'.format(epoch))\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # get new train/test splits\n",
    "            # note: add switch to genLoaders to allow for fixed/random sampling\n",
    "            train_loader, test_loader, _, plot_loader = EcogDataloader.genLoaders(dataset, sample_idx, train_frac, test_frac, valid_frac, BATCH_SIZE, rand_samp=RAND_SAMP, plot_seed=plot_seed_idx)\n",
    "\n",
    "        #     print('Training Network:')\n",
    "            _, trbl_ = Training.train(model, train_loader, optimizer, criterion, CLIP, TFR)\n",
    "            train_batch_loss.append(trbl_)\n",
    "            train_loss[e_idx] = np.mean(trbl_) # this is the plotted training loss\n",
    "        #     print('Testing Network:')\n",
    "            _, tebl_, _ = Training.evaluate(model, test_loader, criterion)\n",
    "            # test_batch_loss.append(tebl_)\n",
    "            test_loss[e_idx] = np.mean(tebl_) # this is the plotted test loss\n",
    "        #     print('Running Figure Sequence:')\n",
    "            plot_loss, plbl_, plot_data_list = Training.evaluate(model, plot_loader, criterion, plot_flag=True)\n",
    "            if not (epoch % 10):\n",
    "        #         print('Saving estimate plots:')\n",
    "                # save the data for the plotting window in dict form\n",
    "                epoch_plot_path = os.path.join(sequence_plot_path,'epoch{}'.format(epoch))\n",
    "                os.makedirs(epoch_plot_path)\n",
    "                torch.save(model.state_dict(),os.path.join(epoch_plot_path,'model_epoch{}.pt'.format(epoch)))\n",
    "                c_list = ['b','r']\n",
    "                for k in range(len(plot_data_list)):\n",
    "                    c_output = c_list[k//n_plot_seed] # blue for training windows, red for testing windows\n",
    "                    plot_data_dict = {\n",
    "                        'src': plot_data_list[k][0],\n",
    "                        'src_dx': plot_data_list[k][1],\n",
    "                        'trg': plot_data_list[k][2],\n",
    "                        'out': plot_data_list[k][3],\n",
    "                        'enc': plot_data_list[k][4],\n",
    "                        'dec': plot_data_list[k][5],\n",
    "                        'srate': srate_down,\n",
    "                        # 'state_dict': model.state_dict(), # putting this in every file is redundant\n",
    "                    }\n",
    "                    torch.save(plot_data_dict,os.path.join(epoch_plot_path,'data_tuple_epoch{}_window{}.pt'.format(epoch,k)))\n",
    "                    # pass data to plotting function for this window\n",
    "                    # for plots:\n",
    "                    # blue: train\n",
    "                    # red: test\n",
    "                    # black: real\n",
    "                    # green: encoder\n",
    "                    # magenta: decoder\n",
    "                    f_out, f_enc, f_dec, f_src = Training.eval_plot(plot_data_dict,c_output=c_output)\n",
    "                    # save plots in current epoch subdir\n",
    "                    f_out.savefig(os.path.join(epoch_plot_path,'output_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "                    f_enc.savefig(os.path.join(epoch_plot_path,'encoder_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "                    f_dec.savefig(os.path.join(epoch_plot_path,'decoder_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "                    f_src.savefig(os.path.join(epoch_plot_path,'source_plot_epoch{}_window{}.png'.format(epoch,k)))\n",
    "                    [plt.close(f) for f in [f_out,f_enc,f_dec,f_src]]\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = Util.epoch_time(start_time, end_time)\n",
    "\n",
    "            if test_loss[e_idx] < best_test_loss:\n",
    "                best_test_loss = test_loss[e_idx]\n",
    "                torch.save({ # this needs to be made into a model class method!\n",
    "                        'epoch': epoch,\n",
    "                        'num_epochs': N_EPOCHS,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_test_loss,\n",
    "                        'data_path': data_file_full_path,\n",
    "                        'train_frac': train_frac,\n",
    "                        'test_frac': test_frac,\n",
    "                        'batch_size': BATCH_SIZE,\n",
    "                        'encoder_length': enc_len,\n",
    "                        'decoder_length': dec_len,\n",
    "                        }, os.path.join(session_save_path,'model_checkpoint.pt'))\n",
    "\n",
    "        #     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        #     print(f'\\tTrain Loss: {train_loss[e_idx]:.3g}')\n",
    "        #     print(f'\\t Test Loss: {test_loss[e_idx]:.3g}')\n",
    "            if e_idx == 0:\n",
    "                ax_loss.plot(e_idx,train_loss[e_idx],'b.',label='train loss')\n",
    "                ax_loss.plot(e_idx,test_loss[e_idx],'r.',label='valid. loss')\n",
    "                ax_loss.legend(loc=0)\n",
    "            else:\n",
    "                ax_loss.plot(e_idx,train_loss[e_idx],'b.')\n",
    "                ax_loss.plot(e_idx,test_loss[e_idx],'r.')\n",
    "            ax_loss.set_ylim(bottom=0,top=1.05*np.concatenate((train_loss,test_loss)).max())\n",
    "            # print the loss curve figure; continuously overwrite (like a fun stock ticker)\n",
    "            f_loss.savefig(os.path.join(session_save_path,'training_progress.png'))\n",
    "            torch.save({'train_loss':train_loss,'test_loss':test_loss,},os.path.join(session_save_path,'training_progress.pt'))\n",
    "    return (train_loss, test_loss, param_session_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,360,237 trainable parameters\n",
      "saving session data to:\t/Volumes/Samsung_T5/aoLab/Data/models/pyt/seq2seq/enc250_dec50_hps_20200523015457227157/model_20200523015457555456\n",
      "[------------------------------------------------------------] 0.0% ...epoch:\t0\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sweep over parameters!\n",
    "# create log file\n",
    "time_str = Util.time_str()\n",
    "out_dir = os.path.join(model_save_dir_path,'enc{}_dec{}_hps_{}'.format(enc_len,dec_len,time_str))\n",
    "os.makedirs(out_dir)\n",
    "result_file = os.path.join(out_dir,'hps_results.csv')\n",
    "with open(result_file,'w') as rf:\n",
    "    rf.write('id,n_layer,n_unit,l_rate,d_rate,train_loss_end,test_loss_end')\n",
    "    rf.write('\\n')\n",
    "for (n_l,n_u,l_r,d_r) in product(n_layers,n_units,l_rate,dropout):\n",
    "    train_loss, test_loss, param_session_name = train_eval_seq2seq(dataset,n_l,n_u,l_r,d_r,out_dir)\n",
    "    with open(result_file,'a') as rf:\n",
    "        rf.write('{},{},{},{},{},{},{}'.format(param_session_name,n_l,n_u,l_r,d_r,train_loss[-1],test_loss[-1]))\n",
    "        rf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecog_is2s",
   "language": "python",
   "name": "ecog_is2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
